---
title: 'Replication of "Why Friends and Neighbors? Explaining the Electoral Appeal of Local Roots"'
author: "Diego Martinez"
date: "4/3/2020"
output: 
  bookdown::pdf_document2
bibliography: [bibliography.bib]



---

# Abstract 

Campbell et al (2019) show that candidate's with shared local roots appeals to voters and that having local roots is an important factor in a voter's candidate selection. I was successfully able to replicate all the results presented in the orignal article "Why Friends and Neighbors? Explaining the Electoral Appeal of Local Roots." by Rosie Campbell, Philip Cowley, Nick Vivyan, Markus Wagner. To further their analysis, I modeled how local roots influence different sub-populations including male vs. female voters as well as between different age groups. I found the average marginal effect of local roots to not remain constant across subgroups, meaning local roots affect groups of voters differently. Thus, any further research regarding local roots effects should consider the gender as well as age makeup of the sample.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# With the time constrictions due to swithcing papers, I was not able to comment
# all of the replication code. I did comment areas I did need to make changes.

library(bookdown)
library(dplyr)
library(tinytex)
library(gt)
library(gtsummary)
library(stargazer)
library(knitr)
library(tidyverse)
library(devtools)
library(sandwich)
library(lmtest)
library(stargazer)
library(cowplot)
library(cobalt)
library(xtable)
library(margins)
library(car)
library(foreign)
library(rms)
library(reshape2)
library(rstanarm)
library(tidybayes)
library(matrixStats)

```



# Introduction

"Why Friends and Neighbors? Explaining the Electoral Appeal of Local Roots" ^[[Replication Paper](https://www.journals.uchicago.edu/doi/abs/10.1086/703131)] takes a deeper look into the "friends and neighbors effect" first researched by Valdimer Orlando Key in 1949. The "friends and neighbors effect" has been well documented over the years; however, the research has been been focused on observing its existence with the reasoning for why it occurs being untested and unknown. The prevailing prior conceptual understanding has been that voters use the fact a candidate has local roots to make assumptions about how a said candidate may represent their constituents. Campbell et al are the first to test this hypothesis, measuring how varying the level of information voter's receive about those running affects the candidate with local roots chances of being elected. They performed a vignette experiment of fake elections instead of analyzing a real election to isolate the effects of local roots and control for the amount of information, not allowing prior information about any candidates influence the results. They modeled respondents' satisfaction with candidates described in the experiment using linear regression, paying attention to the interaction term between local roots and behavioral information. They found results in favor of their hypthoesis as the local roots effect diminished in the presence of more information; however, the result being positive, they took a deeper look with a second study. After performing a conjoint survey experiment, they measured the average marginal component effect, change in probability of a candidate being preferred compared to a baseline for each regressor variable, using an OLS model. The model showed candidates with local roots compared to no local roots to be substantially preferred with this difference being one of the largest out of all the attributes analyzed. Thus, the reasoning for why this "friends and neighbors" effect occurs is not solely that voters using local roots to make inferences, suggesting that local roots on their own is important in candidate selection.

I utilized R, a language and environment for statistical computing, for the replication of the author's code [@r]. The data and code for the paper were made available on  Dataverse [@campbell_cowley_vivyan_wagner_2018]. I followed the format laid out by Gary King in "Publication, Publication" to guide my replication project [@king]. I use many techniques from the Gov 1006 textbook "Regression and Other Stories" [@RAOS] as well. I was able to succesfully replicate all the results of the paper. All of the their code as well as my own can be found in my Github repository ^[[Github Reposotory](https://github.com/diegomartinez1221/gov_1006_replication)]. 

I found the results of the original paper very intriguing; however, my immediate question was how would the results hold across subsets of the sample population. Different age groups as well as genders display different preferences in political issues and voting trends, thus, should it be expected that they react to local roots in the same way? I began by simplying the regression model utilized by the authors in study 1 by excluding the education and social grade variables ^[After performing leave one out cross validation, loo, I found a model without these variables to be just as predictive, meaning these were only adding noise]. I also performed my analysis within a Bayesian framework, using stan_glm instead of just lm for my modeling. In order to look at how local roots affects subsets differently, I interacted the local roots treatment variable with the gender and age group variables. The average local roots treatment effect were slightly different across genders; however, the effect varied signicantly between age groups with the effect being stronger for older voters.

To continue with this theory, I also extended my analysis to the study 2 data. Being a conjoint survey experiment, the best method in which to model such data is using an OLS model. Thus, the only way to compare the local roots effects across genders and age groups was to filter and subset the data prior to running their model. I found that the change in probability of MP being preferred due to local roots for men was about 1/3 less than that for women. In between age groups, I saw similar results to those I found in my extension of study 1. The older voters showed a much stronger affinite for local roots, while younger voters change in probability was more affected by changes in the levels of behavioral information. 



# Literature Review 

The local roots effects has been well studied since 1949. Valdimer Orlando Key was the first to study this for Presidential Elections in southern USA, dubbing his findings as the "friends and neighbors effect" [@Key]. In analyzing Arkansas, he found that in their one party political system that the best explanation for support was just where you are from and who you know. Key himself was concerned by his own findings stating that candidates "can gain support, not primarily for what he stands for or because of his capacities, but because of where he lives” [@Key] . However, what Key found was only observational evidence that "candidates for state office tend to poll overwhelming majorities in their home counties" [@Key]. This friends and neighbors effect was not quantitatively highlighted until 1973 in an analysis of George Wallace's support across his govenitorial elections. In this analysis, counties of Alabama were classified as friends and neighbor counties were ones he had a disprortionate level of support and they all happened to be located around his home county in southeastern Alabama [@black_black_1973]. They found "evidence for the persitence of a strong degree of localism in politics" due to the importance of the the friends-and-neighbors indicator variable in their regression models [@black_black_1973]. A year later Raymond Tatalovich confirmed many of the previous findings in his research of Mississippi as well as made extensions across different types of elections. He found that the most national elections in Mississippi (Senate Races) in fact had the most "friends and neighbors" voting [@tatalovich_1975]. Furthermore, important to my replication paper, he found that "increased visbility of candidates does serve to blunt their friends and neighbors support" [@tatalovich_1975]. 
  
At this point, there was criticism as to the validity of this theory as their had not been anything larger than statewide elections tested and all the testing so far was on Southern states in the USA. The question was were these friends and neighbors results generalizable. Michael Gallagher was the first to document Key's theories outside the US in the importance of localism in candidate selection in Ireland [@gallagher]. Michael Lewis-Beck, Michael and Tom Rice were the first to look at localism on the national level of a presidential election, testing the true presence of the home state advantage. Not only did they find that there is such a thing, but they also most interestingly found that this has not diminished over time, even with the nationalization of elections and greater ease with which candidates were able to reach voters across the USA [@lewis-beck_rice_1983]. The local advantage still endures. A myrad of further studies have been conducted on varying levels of elections from local to state wide to nation wide in a fair amount of different countries including Australia, Canada, Britain, Germany Estona, and Norway. The local roots/"friends and neighbors effect" appears has been proven to be far more than just a local phenomenon in one particular part of the world.
    
The more recent studies have focused on the causes of the friends and neighbors effect instead of just proving its existence. Shaun Bowler Todd Donovan and Joseph Snipp believed the friends and neighbors effect is indirect, coming just out of having more information about candidates due to greater local media coverage for local candidates. Thus, there conclusion were opposed to previous understanding that people use local roots to evaluate candidates. However, they find localism merely comes out of "a voter’s proximity to local media sources" and how "knowledge about candidates is distributed" [@bowler_donovan_snipp_1993]. Similar results were also found in the UK that "the effects result more from the distribution of information than "from the hometown boy infuence" Key origonally proposed [@johnston_wickham-jones_pattie_cutts_pemberton_2016]. However, nothing has been definitive with studies still showing from surveys responses that voters exhibit "small differences in the ratings of candidates in response to sex, religion, age and education cues but more sizeable effects are apparent for the candidate's occupation and place of residence" [@campbell_cowley_2013]. 

Thus, with conflicting evidence, at the time of the paper I am analzying, there was still much unkown about the nature of the friends and neighbors effect that Campbell et al sought to finally answer. Their explanation for the "direct effect of local roots on voter evaluations of a politician" is that "voters rationally use local roots as a low-cost cue for making inferences about a politician’s “behavioral localism" [@campbell_cowley_vivyan_wagner_2019]. This idea of behavioral localism was defined as a politician's willingness to act in the local communities interest [@shugart_valdini_suominen_2005]. This comes out of voters believing that candidates with local roots "are more emotionally connected to the constituency and better informed about constituents’ needs [@shugart_valdini_suominen_2005]. Voters tend to believe local candidates share ideologies, experiences, and will have greater concern for them, thus, they receive the local support out of mere inference. This theory is finally tested in the article I analyze, directly testing whether or not local roots effect is just a filler for lack of other information. They do so by treating voters with behavioral localism information. The quantity measured is the presence as well as the change of magntitude in the friends and neighbors effect when voters are given more information.
    

# Paper Review 

My replication paper will be looking at Rosie Campbell, Philip Cowley, Nick Vivyan, Markus Wagner's paper, "Why Friends and Neighbors? Explaining the Electoral Appeal of Local Roots" ^[[Replication Paper](https://www.journals.uchicago.edu/doi/abs/10.1086/703131)] published in The Journal of Politics. Whereas the "friends and neigbors" effect has previously been found to arise when there is a lack of information in regard to policy, the only thing voters know is they are local, this article's purpose is to explain the "direct effect of local roots on voter evaluations of a politician" [@campbell_cowley_vivyan_wagner_2019]. It is the authors belief that voters "use local roots as a low-cost cue for making inferences about a politician’s “behavioral localism" (extend to which an elected official acts in the interest of their local consitutients)[@campbell_cowley_vivyan_wagner_2019]. Their hypothesis is that the power of local roots in decision making for voters is weakened when voters are more informed about their behavioral localism. The authors use survey experiments of made up candidates running for Parliament in the UK rather than performing an observational study on real elections. Thus, they are able to isolate the effects of local roots, ensuring prior knowledege is not a factor. Everyone in the experiment will have the same level of knowledge about each candidate given to them in biographical descriptions through the survey. During real elections, a cofounding variable of the local roots effect is the disproportionate amount of local media coverage for local candidates leading to easier recognition. 

Study 1 used a vignette experiment to more generally test whether the presence of information on behavioral localism impacts the effects of local roots while Study 2 gives people a more wide array of information to test whether having greater knowledge of each candidate eliminates the effects of local roots. Study 1 presented a hypothetical election between two candidates, testing how varying levels of behavioral localism information interacts with being a local candidate. The election was between Phillip (the constant), who moved to the area five years ago, and Nick whose localness was subject to change and assigned randomly. The experiment was set up in such a way that respondents were randomly given either no behavioral information or a short description for each candidate. When behavioral information was provided, Nick's behavior also varied between low (spending majority of time on national issues) and high behavioral localism (spending majority of time on local issues) while Phillip always had a constant average behavioral localism (splits time each week between dealing with local issues vs. dealing with national issues). With the information provided, respondants were asked on a scale from 0 to 10 (10 being most satisfied), how satisfied would they be with having each candidate as their elected official. The depedent variable for the model was Nick's score minus Phillip's. The authors modeled this with a least squares regression model and their focus was on the interaction term (being a local candidate and behavioral information presented). The results were that the presence of behavioral information whether positive or negative for Nick does lessen the friends and neighbors effect, agreeing with their hypothesis that local roots is a cue when no other information is provide. However, they found Nick being a local candidate still positively impacts voters as the average treatment effect was positive and significant. The authors thus took a deeper look into this with study 2. 

In study 2, every respondent was treated with information on behavioral localism, local roots, and other varying characteristics such as gender, political interests, and party. Thus, in contrast to study 1, this will test if the local roots effect is nullfied in the candidate selection process when a wealth of other knowledege is presented to the voters, leaving them without the need to use local roots as a cue for the unknown. Each respondent was given five hypothetical elections with randomly varying attributes, all of which were more specific than in study 1. The respondents were asked to pick wihch candidate they preferred based on information about the localness of the candidates, behavioral localism measured by on how much time is spent on local issues and where the candidates allegiance lies (national party, constituents, or personal views), and other defining characterstics like age, gender, and policy interests. Since study 2 posseses much more and much more varied information on the candidates in question, they analyze the average marginal component effect (AMCE) using an OLS model. This measures the probability of why a candidate is being chosen based on the candidate's particular attribute for each category compared to a baseline. This will present the change in probability of a candidate being preferred broken down by each possible indivual component of their profile. Even here with so much more information, they found the AMCE for being local was positive and one of the most prominent compared to other categories as well as the baseline of not being local. Thus, the authors empirically found local roots to be an important aspect for voter's in candidate selection and it is not merely a cue. 


# Replication 

I was able to replicate all the results of the paper. I came across a few errors based on the updating of packages; however, these were easily fixed. I had the most difficulty trying to replicate the aesthetics of the figures and tables, particularly the stargazer table. I was able to replicate all the numerical results; however, the style and formatting were not exactly the same. My main issues were with the formatting of captions and notes after a table or figure. I turned to the bookdown package to handle adding the neccessary captions. 

# Extensions 


My immediate question after reading the article was how would the results hold across subsets of the sample population. For study 1, the authors in one of the appendices performed balance and randomization checks to ensure that there is a "distributional balance of four respondent characteristics – gender, age, education and social grade, all measured pretreatment – across the six treatment groups created in the experiment" [@campbell_cowley_vivyan_wagner_2019]. Since "none of the differences in proportions across treatment groups are bigger than 10% and only a small number are greater than 5%" [@campbell_cowley_vivyan_wagner_2019], this presents an excellent opportunity to dive deeper into if the "friends and neighbors" effect is stronger for one subset of the population than others. I hypothesis that there may in fact be differences, especially in regard to age and gender. For example, do the younger people in the study, who may not be as engaged in politics or care how they are represented in Parliament, more often vote for the candidate with local roots. Or even on the opposite end of the spectrum, do the elder people in the study, who have been engrained in their community and care about their representation, vote for the local candidate? Different age groups as well as genders display different preferences in political issues and voting trends, thus, should it be expected that they react to local roots in the same way?

The authors built four different linear regression models to analyze the data from study 1; however, they focused on one for the entire project. They used this particular model to analyze the average treatment effect as well as make predictions ^[Repliction Appendix a. - They solely used model 4. Differed from the first two as different levels of behavioral information were interacted with the local roots effect whereas the others only interacted with the presence of additional information. Differered from model 3 as model 3 simply looked at the interaction while model 4 included respondent characteristic variables]. I compared the model they used which included all four of the respondent characterstics variables with the simple model that only included the interaction between local roots and behavioral information, using leave one out cross validation. To perform such an analysis, I needed to utilize stan_glm instead of lm as the authors did. I found the model without the extra variables to be only slighlty inferior, meaning these variables were potentially only adding noise. ^[Extension Appendix a for full results of the loo comparison]. Still wanting to subset the results, I simplified the regression model by excluding the education and social grade variables (Extension Appendix b). I only used this model to test that I would get similar results using stan_glm instead of lm. In figure 3 (extension appendix c), the predictions ^[Predictions for Nick minus Phill's scores analyzed at the modal value of each category (females, aged 25-49)] using stan_glm and posterior_linepred mirrored the predictions displayed in figure 1 of the original paper (replication appenix b).

In order to look at how local roots affects subsets differently, I interacted the local roots treatment variable with the gender and age group variables. The average local roots treatment effect ^[Since I am using stan_glm and posterior_linpred, the regression models as well as predictions are based off of thousands of simulations. Thus, I chose to use the median and mad instead of mean and standard deviation as they ar emore stable with simulated results.] were slightly different across genders; however, the effect varied signicantly between age groups with the effect being stronger for older voters (extension appendix d). For all subgroups, the the effects of local roots on voter evaluations of politicians are reduced when voters receive direct information. However, my findings differ in tha fact that local roots do not always have a significant positive impact. For the youngest category, 18-24, even with no behavioral information provided, the local roots effect is negligible for females. However, contrasting this, the local roots effects are even stronger than those reported in the article for the older members of the sample population. Thus, it should not be assumed that the local roots effect is homogenous with such a wide range of average treatment effects across gender and age subgroups.  

To continue my analysis, I also wanted to frame my findings using the context of study 2. With study 2 being a conjoint survey experiment, the best method in which to model such data is OLS. Thus, I compare the local roots effects across genders and age groups by filtering the data prior to running the separe OLS models. Figure 4 (extension appendix e) shows the results when subsetting by gender and figure 5 shows the results when subsetting by age. In line with my previous findings, the changes between groups were substantial.The change in probability of a candidate being preferred due to local roots for men was about 1/3 less than that for women. In between age groups, I also saw that older voters showed a much stronger affinite for local roots, while younger voters change in probability was more affected by changes in the levels of behavioral information. From the original study, MPs with any level of local roots were 10% and 13% more likely to be chosen by respondents. From my tests, males showed a lower response of only about a 10% increase for the 20 year as well as grew up in the area levels of local roots whereas the effect on females exceeded 15 %. The results between the age groups were much more varied and there was not a distinct trend in the older a voter is the more likely they will chose a candidate based on local roots. However, all the results agreed with those of the original article in that the magnitude of the local roots effect is consistantly amongst the largest across all the attributes. 

Thus, I got mixed results in comparison to those documented in the original article as well as between my two different extensions. The conclusion that the presence of behavioral information negatively impacts the magnitude of the local roots effect is consistent across all my findings. However, the conclusion that local roots have a positive and significant effect do not hold constant across subsets of the population. The treatment effect of local roots varied substantially across age subgroups from the study 1 data, ranging from being not significant for the younger groups to being even large than the average treatment effect presented in the article. However, interestingly, the results of for my extension of study 2 showed opposite findings. Here, the difference in the average marginal component effect for females was greater than for males, while the differences across age subsets were smaller. Thus, there are definitely differences in the local roots effect and it cannot be assumed to be held constant across the entire electorate. More research should be performed directly on why local roots effect males vs. females as well as older vs. younger voters. 






# Replication Appendix 



```{r, table 1 setup}
# Replication code for Study 1 in 
# Campbell, R., Cowley, P., Vivyan, N & Wagner, M. 
# 'Why friends and neighbors? Explaining the electoral appeal of local roots'.
# Journal of Politics


### Load in Study 1 data

d <- readRDS("./friends_neighbors_replication/study1data.rds")


### Table 2

# The authors created various different models of the data. These models are
# displayed in table 1 of the paper and model 4 is used throughout the paper.
# The authors same the results of the models and the errors in separate objects
# so that they can all be called togethered to create a stargazer table

m1 <- lm(nickminusphil ~ localtreat*behtreatsimple, data = d)

se1 <- sqrt(diag(vcovHC(m1)))

m2 <- lm(nickminusphil ~ localtreat*behtreatsimple +
           gender + agegrp + socgrade + qual
         , data = d)
se2 <- sqrt(diag(vcovHC(m2)))

m3 <- lm(nickminusphil ~ localtreat*behtreat, data = d)
se3 <- sqrt(diag(vcovHC(m3)))

m4 <- lm(nickminusphil ~ localtreat*behtreat +
           gender + agegrp + socgrade + qual
         , data = d)
se4 <- sqrt(diag(vcovHC(m4)))


#loo(m1)


#stan_m3 <- stan_glm(nickminusphil ~ localtreat*behtreat, data = d, refresh = 0)

#stan_m4 <- stan_glm(nickminusphil ~ localtreat*behtreat +
         # gender + agegrp + socgrade + qual
         #, data = d, refresh = 0)


#stan_m5 <- stan_glm(nickminusphil ~ localtreat*behtreat + age + age*localtreat*behtreat
         #, data = d, refresh = 0)


#stan_m6 <- stan_glm(tnickgt ~ localtreat*behtreat +
          # gender + agegrp + socgrade + qual
         #, data = d_new, family = binomial, refresh = 0)


#stan_m7 <- stan_glm(tnickgt ~ localtreat*behtreat +
          # gender + agegrp 
         #, data = d_new, family = binomial, refresh = 0)




#l_1<-loo(stan_m3)

#l_2<-loo(stan_m4)

#l_3<-loo(stan_m5)

#loo_compare(l_3, l_2)

#d_new<- d %>% mutate(tnickgt = case_when(nickgt == "TRUE" ~ 1, TRUE ~0))
```


```{r table 1, results="asis",  message=FALSE, comment=FALSE}
## Output to table

# I needed to change/add many things to the original to get a more similar table
# to the one that apppars in the paper. 

stargazer(header = FALSE, 
          
          # calling all the linear models and errors created in teh previous chunk. 
          
          mget(paste0("m",1:4)),
          se = mget(paste0("se",1:4)),
          
          # latex was the only kind I could get to output in the pdf. It was
          # orignally type html
          
          type = "latex",
          
          # needed to add the title myself as well. 
          
          title = " Relative Ratings of MP Nick by Local Roots and Behavioral Information Treatments in Study", 
          
          # Could not find another way to add the two separations visible in
          # the table model. I needed to use these latex commands combined with
          # column.separate.
          
          column.labels=c('\\shortstack{Conditioning Effect of Any Behavioral\\\\ Localism Information}', '\\shortstack{Separate Conditioning Effects for\\\\ High and Low Behavioral Localism}'),
          column.separate = c(2,2),
          
          # both of these are needed to erase the dependent variable 
          
          dep.var.labels.include = FALSE,
          dep.var.caption = "",
          #intercept.bottom = FALSE, intercept.top = TRUE,
          keep.stat = c("n", "rsq", "adj.rsq"),
          omit = "socgrade|agegrp|gender|qual",
          order = c("Constant", "^localtreatLocal roots$", 
                    "^behtreatsimpleBehavioural info$", 
                    "^behtreatConst. focus$", 
                    "^behtreatWestmin. focus$",
                    "^localtreatLocal roots:behtreatsimpleBehavioural info$",
                    "^localtreatLocal roots:behtreatConst. focus$",
                    "^localtreatLocal roots:behtreatWestmin. focus$"
          ),
          add.lines = list(c("Controls for voter characteristics?", 
                             rep(c("No","Yes"), 2))),
          covariate.labels = c("Intercept", "Local roots", "Behavioral localism information",
                               "Behavioral localism: High (vs. no info)",
                               "Behavioral localism: Low (vs. no info)",
                               "Local roots X Behavioral info.", 
                               "Local roots X High behavioral localism",
                               "Local roots X Low behavioral localism"),
          
          # The notes section was not originally included in the replication
          # code. I was able to create something similar using the latex
          # commends, but it is not an exact match
          
       notes.append = FALSE, notes.align = "l",
      notes.label  = "\\multicolumn{5}{l}{\\parbox[t]{\\textwidth}{All models estimated via ordinary least squares. Dependent variable is respondent relative rating of MP Nick (the 0–10 rating of Nick minus that of Philip). Robust standard errors in parentheses. N = 5,203. * p <= .1.,** p <= .05., *** p < .01.}}\\\\",
      single.row = TRUE,
      column.sep.width = "1pt")

```


\hfill\break
\hfill\break
\hfill\break
\hfill\break
\hfill\break
\hfill\break



```{r figure 1 setup, cache=TRUE}
### Figure 1

### Create useful functions

## Function to get predicted levels for different treatment combinations.


# function to make predictions from the linear regression on a person with
# different attributes. very much like postior_linpred.the Inputs for the
# function are a regression object, covariate matrix for all the vairables in
# the regression object, and the new data to perform predictions.

predict.rob <- function(object, vcov,newdata){
  
  # extracting info from the regression object
  
  tt <- terms(object)
  
  # check to make sure that the user input a new dataset
  
  if(missing(newdata)){ newdata <- x$model }
  else {
    
    # takes the response variable, the y, out of the terms object
    
    Terms <- delete.response(tt)
    
    # creates a dataframe were all the variables are constant save the local
    # treatment and behavioral treatment. Each row of the matrix is a different
    # combination of the two treatments.
    
    m <- model.frame(Terms, newdata, na.action = na.pass, 
                     xlev = object$xlevels)
    if (!is.null(cl <- attr(Terms, "dataClasses"))) 
      .checkMFClasses(cl, m)
    
    # turns the dataframe m into a dataframe of 1's and 0's for each row of m.
    # This now resembles how m will be fed into a regresson. For example the
    # Intercept is 1 for every row. However, for rows of m where there was an
    # interactions, the value will be 1 and when the interaction did not take
    # place, there will be a 0. Thus, it is a sort of expanded version of the
    # regression equation.
    
    X <- model.matrix(Terms, m, contrasts.arg = object$contrasts)
    
    # checks for if any of the variables are NULL from the original m matrix.
    # this adds in 0's for those instances where there are NULLs.
    
    offset <- rep(0, nrow(X))
    if (!is.null(off.num <- attr(tt, "offset"))) 
      for (i in off.num) offset <- offset + eval(attr(tt, 
                                                      "variables")[[i + 1]], newdata)
    if (!is.null(object$call$offset)) 
      offset <- offset + eval(object$call$offset, newdata)
    mmDone <- FALSE
  }
  #m.mat <- model.matrix(x$terms,data=newdata)
  
  # saving the coefficients from the model 
  
  m.coef <- object$coef
  
  # performing matrix multiplication to get the predicted value for each row,
  # which is each a set person with set attributes experiencing each of the
  # different treatments. Also does the same for the standard erorrs. 
  
  fit <- as.vector(X %*% object$coef)
  se.fit <- sqrt(diag(X%*%vcov%*%t(X)))
  
  # finally the function combines the results of fit and se.fit to lists as the
  # output.
  
  return(list(fit=fit,se.fit=se.fit))
}

## author: Avg treatment effect of local roots with const and westmihn. info

# the margins command will collect the average treatment effect for each of the
# covaraites at every level of behavioral info. This is the crux of the
# analysis. The one that is important to us is the avg effect for
# localtreatLocal roots. Thus subetting to only keep the effects when local
# treatment interacts with the levels of behavioral info.

out <- summary(margins(m4, vcov = vcovHC(m4), 
                       at = list(behtreat = c("No behavioural info", "Const. focus", "Westmin. focus"))))
out <- subset(out, factor == "localtreatLocal roots")

margins.m4 <- out 


## make margins plot
margins.comb <- margins.m4

# changing the variables to be more informative in the graphic. Most were
# abbreviated to save space in the original dataset

margins.comb$behtreat.neat <- car:::recode(margins.comb$behtreat, 
                                           '"No behavioural info" = "Behavioral information treatment --\\nNo information (Vignettes 1-2)";
                                           "Westmin. focus" = "Behavioral information treatment --\\nLow behavioral localism (Vignettes 5-6)";
                                           "Const. focus" = "Behavioral information treatment --\\nHigh behavioral localism (Vignettes 3-4)"')
                                           #as.factor.result = TRUE)
margins.comb$behtreat.neat <- factor(sub(" --", ":", margins.comb$behtreat.neat),
                                     levels = c("Behavioral information treatment:\nNo information (Vignettes 1-2)", 
                                                "Behavioral information treatment:\nLow behavioral localism (Vignettes 5-6)",
                                                "Behavioral information treatment:\nHigh behavioral localism (Vignettes 3-4)"
                                     ))

# the first grahic has two parts that are set side by side. Thus, it is
# necessary to save the plot into an object p1.

p1 <-
  
  # plotting the average treatment effect when a candidate is local for each
  # level of behavioral info; however behavioral info is not yet incorporated
  # into the graph.
  
  ggplot(margins.comb, aes(x = factor, y = AME)) + 
  
  # line at 0 to show all are results are significant
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # show range of the effect 
  
  geom_linerange(aes(x=factor, ymin=lower, ymax=upper), size = 0.6) +
  
  # point estimate for the averages 
  
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  
  # flipping the coordinates because factors is currently all the same. Also
  # aesthetically more pleasing to go horizontally
  
  coord_flip() +
  
  # needing the facet wrap to add the info for each of the behavioral levels.
  # Currently everything was graphed on same axis and the lines were on top of
  # each other. This separates the treatment into each combo of local roots with
  # behavioral roots.
  
  facet_wrap( ~ behtreat.neat, ncol = 1) + 
  
  # manipulating aesthetics and adding a title for the final output.
  
  theme_bw() +
  theme(legend.position = "bottom") +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + 
  ggtitle("(b) Effect of MP local roots \ntreatment") 

## predlevels for m4

# dataframe created for predictions for the predict.rob function we made
# earlier. The table will consistent of a combinations of all local roots and
# behavioral treatments levels (2,3 so 6 row in total). The table then then is
# filled with the most common value for each of the voter characterstics (most
# common age, gender...). Thus the table is one of the treatments effects on the
# most common of individual from the sample.

newdf <- data.frame(expand.grid(localtreat = factor(c("No local roots", "Local roots"), 
                                                    levels = levels(d$localtreat)),
                                behtreat = factor(levels(d$behtreat), levels = levels(d$behtreat))
),
gender = factor(levels(d$gender)[which.max(table(d$gender))], 
                levels = levels(d$gender)),
agegrp = factor(levels(d$agegrp)[which.max(table(d$agegrp))], 
                levels = levels(d$agegrp)),
socgrade = factor(levels(d$socgrade)[which.max(table(d$socgrade))], 
                  levels = levels(d$socgrade)),
qual = factor(levels(d$qual)[which.max(table(d$qual))], 
              levels = levels(d$qual))
)



# using the function created above to get predictions for each of the rows of
# newdf. The output of pred.rob saved in preds is a list. The next step is
# adding those values into the newdf dataframe to make the second part of
# graphic 1.

preds <- predict.rob(m4, vcov = vcovHC(m4), newdata = newdf)
newdf$yhat <- preds$fit
newdf$se.yhat <- preds$se.fit
newdf$lo <- newdf$yhat - (1.96*newdf$se.yhat)
newdf$hi <- newdf$yhat + (1.96*newdf$se.yhat)
predlevels.m4 <- newdf


## make predlevels plot

# releveling the variables to be more informative in the graphic. Most were
# abbreviated to save space in the original dataset

predlevels.comb <- predlevels.m4
predlevels.comb$behtreat.neat <- car:::recode(predlevels.comb$behtreat, 
                                              '"No behavioural info" = "Behavioral information treatment --\\nNo information (Vignettes 1-2)";
                                              "Westmin. focus" = "Behavioral information treatment --\\nLow behavioral localism (Vignettes 5-6)";
                                              "Const. focus" = "Behavioral information treatment --\\nHigh behavioral localism (Vignettes 3-4)"')
                                              #as.factor.result = TRUE)
predlevels.comb$behtreat.neat <- factor(sub(" --", ":", predlevels.comb$behtreat.neat),
                                        levels = c("Behavioral information treatment:\nNo information (Vignettes 1-2)", 
                                                   "Behavioral information treatment:\nLow behavioral localism (Vignettes 5-6)",
                                                   "Behavioral information treatment:\nHigh behavioral localism (Vignettes 3-4)"
                                        ))
# second plot is pretty much identical to the first plot created. This one is
# just with predicted values for the most common voter profile instead of real
# data.

p2 <-  ggplot(predlevels.comb, aes(x = localtreat, y = yhat)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_linerange(aes(x=localtreat, ymin=lo, ymax=hi), size = 0.6) +
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  coord_flip() +
  facet_wrap( ~ behtreat.neat, ncol = 1) + 
  theme_bw() +
  theme(legend.position = "bottom") + 
  labs(title = "(a) Predicted relative rating")


## Now combine aspects of above plots into 3 x 2 plot
pcomb <- plot_grid(p2, p1, align = "h", rel_widths = c(1.0,0.80))

```


```{r fig.cap = 'Effects of local roots conditional on behavioral information treatments (study 1). A, Predicted relative rating of MP Nick (MP Nick rating minus MP Philip rating) as the MP local roots treatment varies, with all control variables held constant at their modal value in the sample. Top, predicted values when respondents receive no information about MP behavioral localism. Middle, predicted values when respondents receive information about MP behavioral localism and Nick is revealed to be low in behavioral localism. Bottom, predicted values when respondents receive information about MP behavioral localism and Nick is revealed to be high in behavioral localism. For each of the same behavioral localism conditions, B show the estimated treatment effect of MP Nick having local roots. Estimates are calculated from model 4 in table 2. Open circles indicate point estimates. Lines denote 95% confidence intervals.'}

pcomb
```

\hfill\break
\hfill\break
\hfill\break
\hfill\break
\hfill\break
\hfill\break

```{r figure 2 functions}

    
## Function to get regression-based AMCE estimates for each attribute level using 
## OLS estimator with clustered SEs (Hainmueller, Hopkins and Yammamoto 2014)

# function called within amce.tab to get the actual amce's 

get.amcetab <- function(data, variables, J = 2){    
  
  # important to have the list of variables length to know how many AMCE's to
  # calculate
  
  Nvar <- length(variables)
  amce.list <- vector("list", length = Nvar)
  
  for(i in 1:Nvar){ # get AMCE for each variable attribute
    
    # regression on every variables with mp.preferred individually. fmla builds
    # each regression through each iteration of the loop
    
    fmla <- as.formula(paste("mp.preferred ~ ",variables[i], sep = ""))
    
    # fmla is placed into ols. Runs a linear regression based on the formula.
    # Gets same results for the regression as just calling lm but there are
    # other things in the output as well
    
    model <- ols(fmla, data = data, x = T, y = T) 
    # NOTE: The data for the model has to have no NAs on any model variables 
    # for the robcov(cluster()) function to work 
    
    #adjusts the variance-covariance matrix of a fit to correct for
    #heteroscedasticity and for correlated responses from cluster samples.
    #Clusters by ID, which is each individual person tested as they were tested
    #10 times.
    
    model.clus <- robcov(model, cluster = data$ID, method = "efron")
    
    # list of the coefficients from the model. taking out the baseline because
    # they do comparisons to the baseline
    
    coef <- model.clus$coef[names(model.clus$coef)!="Intercept"]
    
    # calculates as well as gets the standard error for each of the coefficients 
    
    se.clus <- sqrt(diag(model.clus$var))
    se.clus <- se.clus[names(se.clus)!="Intercept"]   
    
    # creating a table from the info drawn from the model 
    
    sub.tab <- data.frame("AMCE" = coef, 
                          "ci.lo" = coef - (1.96*se.clus),
                          "ci.hi" = coef + (1.96*se.clus),
                          "cluster.se" = se.clus)
    
    # making the name of each of the coefficients from the model a column in the
    # created dataframe
    
    sub.tab$category <- names(coef)
    
    # splits into two columns. Since these are categorical variables, the
    # rgression output is the name of the variable being regressed on, and equal
    # sign, and then the category. This separates the variable and the category
    # of the coefficient into two columns
    
    sub.tab <- cbind(sub.tab, colsplit(sub.tab$category, "=", c("attribute","level")))
    sub.tab$level <- as.character(sub.tab$level)    
    
    # no longer needed since made the two columns out of the variable names
    
    row.names(sub.tab) <- NULL
    
    ## add in gaps and baselines
    
    # these had been taken out when we did not want coefficient for the
    # intercept. Now it is added back into table for reference as 0's
    
    to.add <- data.frame(AMCE = c(NA,NA, 0), ci.lo = c(NA,NA, 0), ci.hi = c(NA,NA, 0),
                         cluster.se = c(NA,NA, 0),
                         category = rep("", 3), attribute = rep(sub.tab$attribute[1],3),
                         level = c("", " ", "baseline"))
    
    # since this is a for loop will do the same for each regression
    # individually. All are saved into amce.list
    
    amce.list [[i]] <- rbind(to.add, sub.tab)
  } 
  
  # create one large table from the list of individual subtables for each regression 
  
  amce.tab <- do.call("rbind", amce.list)
  
  # re-make initial labels column
  
  amce.tab$category <- paste(amce.tab$attribute, amce.tab$level, sep = ": ")
  
  # make this into ordered factor
  amce.tab$category <- factor(amce.tab$category, levels = rev(amce.tab$category), order =T)    
  
  return(amce.tab)
}


## author: Function that calls get.amcetab for multiple predictors and combines results

# inputs for the function are a dataframe, variables from dataframe wanting to
# be used to get their amce and whether there are multiple datasets, and if they
# test subjects are in same political party.

# amce.tab only function is to prepare data so as to call get_amcetab. That is
# why there are many if statements for different number of datasets and parties.

amce.tab <- function(data, variables, multi = F, same.party = F){
  # data must be a single data frame or a list of data frames (if multi = T)
  # with named elements
  # Also relies on specific ordering of explanatory variables

  # first try when multi is true. What is going on here is iterating over the
  # list of dataframes all calling get_amcetab for each of the dataframes in the
  # list. Necessary that all dataframes have the variables inputed into the
  # orignal function
  
  if(multi == T & same.party == F){
    amce.tab.list <- list(NA, length = length(data))
    for(i in 1:length(data)){
      tmp <- get.amcetab(data[[i]], variables = variables)
      tmp$set <- rep(names(data)[i], nrow(tmp))
      amce.tab.list[[i]] <- tmp
    }
    
    # combining all the dataframes together to create one large dataset with set
    # column to be able to tell which amce belongs to which dataset from the list.
    
    amce.tab <- do.call("rbind",amce.tab.list)
    amce.tab$set <- factor(amce.tab$set)
    return(amce.tab)
  }
  

  # same as before in needing to iterate over every dataset in the list of
  # datasets and call get_amcetab for all of them
  
  if(multi == T & same.party == T){
    amce.tab.list <- list(NA, length = length(data))
    for(i in 1:length(data)){
      
      # check for same party. If they are the same party. If true, the first
      # variable is taken out of the variable list. This is why author stated
      # that the order of variables is important.
      
      vars <- if(grepl("same party", names(data)[i])==T|grepl("Same Party", names(data)[i])==T) variables[2:length(variables)] else variables
      
      # regardless now calls get.amcetab for every element of the list
      
      tmp <- get.amcetab(data[[i]], variables = vars)
      tmp$set <- rep(names(data)[i], nrow(tmp))
      amce.tab.list[[i]] <- tmp
    }
    names(amce.tab.list) <- names(data)
    
    # separtes the data where they are same party and datasets where they
    # are not in same party
    diff.party <- amce.tab.list[grepl("same party", names(amce.tab.list))==F&
                                  grepl("Same Party", names(amce.tab.list))==F]
    same.party <- amce.tab.list[grepl("same party", names(amce.tab.list))==T |
                                  grepl("Same Party", names(amce.tab.list))==T]
    
    # dataframe to fill in when they are same party 
    
    to.add <- data.frame(AMCE = rep(NA, 4), ci.lo = rep(NA, 4), ci.hi =  rep(NA, 4),
                         cluster.se =  rep(NA, 4),
                         category =  diff.party[[1]]$category[1:4], 
                         attribute =  diff.party[[1]]$attribute[1:4], 
                         level = diff.party[[1]]$level[1:4],
                         set = rep(NA, 4))
    
    for(i in 1:length(data)){ 
      
      #replacing results from amcetab when same party is true.  
      
      if(grepl("same party", names(data)[i])==T|grepl("Same Party", names(data)[i])==T){
        amce.tab.list[[i]] <- rbind(to.add,amce.tab.list[[i]])
        amce.tab.list[[i]]$set[1:4] <-  amce.tab.list[[i]]$set[5:8]
      }
      else amce.tab.list[[i]] <- amce.tab.list[[i]]
    }    
    
    # finally combines all the datasets from the list of datasets together and
    # outputs the results together.
    
    amce.tab <- do.call("rbind",amce.tab.list)
    amce.tab$set <- factor(amce.tab$set)
    return(amce.tab)
  } 
  
  # last if statement and it is the easiest just when multi is false. All
  # amce.tab needs does is just call get.amcetab
  
  if(multi == F)   {
    
    get.amcetab(data, variables)
  }
  
}


```




```{r message=FALSE, warning=FALSE, fig.cap= 'Estimated average marginal component effects of each MP attribute level compared to the baseline level of the attribute, estimated via ordinary least squares regression, with standard errors clustered by respondent. Bars show 95% confidence intervals.'}
### Load in Study 2 data 

long.dat <- readRDS("./friends_neighbors_replication/study2data_long.rds")
wide.dat <- readRDS("./friends_neighbors_replication/study2data_wide.rds")


### Create labels for plotting

# for full results

# more informative labels 

labels.full <- rev(expression(
  "", italic("Party & position (baseline = Labour left-wing)"), "Labour centre", "Conservative centre", "Conservative right-wing",
  "", italic("Local roots (baseline = lives elsewhere)"),"5 years in area", "20 years in area", "Grew up and lives in area", 
  "", italic("Constituency work (baseline = 1 day)"), "2 days", "3 days", "4 days",
  "", italic("Main policy influence (baseline = party)"), "constituents' views","own personal views",
  "", italic("Policy interests (baseline = economy and tax)"), "education and health",
  "", italic("MP sex (baseline = female)"), "male"))

# for analysis of local roots only
labels.sub <- expression("Grew up and lives in area", "20 years in area", "5 years in area", 
                         italic("Local roots (baseline = lives elsewhere)"))


# label for x axis

effect.label <- "Change in probability of MP being preferred,\n relative to baseline"


### Figure 3: AMCEs for all attributes

# calling amce.tab to get the amce values for all these variables when used as
# explanatory variable for mp.preferred

res <- amce.tab(data = long.dat, 
                variables = c("mp.partypos", "mp.localroots", "mp.const", "mp.influence", "mp.policy", "mp.gender")
                , multi = F)

# reversing the order of the factors for the various categories of amce, unsure
# why

res$category <- factor(as.character(res$category), levels = rev(as.character(res$category)), order =T)
#write.csv(res, "amce-all.csv")# write results to csv file

# Full plot for all attributes

# since each had two spaces in between, the top one is not needed.

res <- res[2:nrow(res),] # chop off top empty layer
res <- subset(res, level != "baseline")# remove artificial 'baseline' rows

# labels created above

labels <- labels.full

# initiates the plot. The attribute column is each variable which is why every
# category within a variable has same color.

ggplot(res, aes(x = category, y = AMCE, color = attribute)) + 
  
  # baseline of 0 to show how significant each coefficient is 
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # gives the standard error range and a point at the amce
  
  geom_pointrange(aes(ymin = ci.lo, ymax = ci.hi), size = 0.75) +
  
  # inputs better label for y axis (Will become x axis)
  labs(y = effect.label, x = "") + 
  
  # With the categories on x axis, everything is jumbled, thus flipping the
  # coordinates so the categories are legible
  
  coord_flip() + 
  
  # adding nice formating 
  
  theme_bw() + 
  theme(axis.text = element_text(colour = "black")) +
  theme(legend.position = "none") +
  theme(text = element_text(size = 15)) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 1), # remove ticks and justify
        axis.title.x = element_text(size = 13, vjust = 0)) + 
  
  # change labels from those from the dataset that were abbreivated to something
  # more infomrative
  
  scale_x_discrete(labels=labels)
```




```{r Stan Extensions, cache= TRUE}


# same models as those created by the authors just ins stan instead of lm

stan_m3 <- stan_glm(nickminusphil ~ localtreat*behtreat, data = d, refresh = 0)

stan_m4 <- stan_glm(nickminusphil ~ localtreat*behtreat +
          gender + agegrp + socgrade + qual
         , data = d, refresh = 0)

#print(stan_m3, digits = 2, detail = FALSE)
#print(stan_m4, digits = 2, detail = FALSE)


# using leave one out cross validation to compare the models 

l_1<-loo(stan_m3)

l_2<-loo(stan_m4)


# models I create to test the effects of age and gender 
stan_m5 <- stan_glm(nickminusphil ~ localtreat*behtreat + agegrp + gender
         , data = d, refresh = 0)



l_3<-loo(stan_m5)

#(l_4<-loo(stan_m6))

# craeting a dataframe with every possible combination of local treatment,
# behavioral treatment, age, and gender to make predictions

newdf_2 <- data.frame(expand_grid(localtreat = unique(d$localtreat), behtreat = unique(d$behtreat), agegrp = unique(d$agegrp), gender = unique(d$gender)))

# making my predictions, posterior_linpred runs 400 simulations for each row of
# newdf_2

probs<-posterior_linpred(stan_m5, newdata = newdf_2, transform = TRUE)

# collect the column median and madinstead of mean and se for each column of
# predictions because the means are more stable

predictions<-data.frame(predictions = colMedians(probs), mad = colMads(probs))



# adding the predictions to the newdf_2 to know the variables that go with the
# row predictions

preds<-newdf_2%>%
  cbind(predictions)%>%
  
  # calculating the 95% confidence intervals 
  
  mutate(lower = predictions - 1.96*mad, upper = predictions + 1.96*mad)

```

```{r, fig.cap= "Presented is the predictions for the nick minus phil ratings for the modal value of each category in the datset, comparing the results between local and no local roots.. This is a method that the authors use and the modal values are a women age 25-49. The purpose of the graphic is to ensure I get similar results when I use stan_glm instead of lm as well as if I get similar predictions taking out the education and socail grade variables. The predictions are concurrent with those found in the orignal paper and we are able to see the diminishing effect of local roots when behavioral information is present."}



preds %>%
  filter(agegrp == "25-49", gender == "Female")%>%
  # plotting the average treatment effect when a candidate is local for each
  # level of behavioral info; however behavioral info is not yet incorporated
  # into the graph.
  
  ggplot(aes(x = localtreat, y = predictions)) + 
  
  # line at 0 to show all are results are significant
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # show range of the effect 
  
  geom_linerange(aes(x=localtreat, ymin=lower, ymax=upper), size = 0.6) +
  
  # point estimate for the averages 
  
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  
  # flipping the coordinates because factors is currently all the same. Also
  # aesthetically more pleasing to go horizontally
  
  coord_flip() +
  
  # needing the facet wrap to add the info for each of the behavioral levels.
  # Currently everything was graphed on same axis and the lines were on top of
  # each other. This separates the treatment into each combo of local roots with
  # behavioral roots.
  
  facet_wrap( ~ behtreat, ncol = 1) + 
  
  # manipulating aesthetics and adding a title for the final output.
  
  theme_bw() +
  theme(legend.position = "bottom") + 
  labs(title = "Predictive Difference in Rating for Modal Level of Each Variable")

```


```{r, fig.cap= " Shows the estimated treatment effect of MP Nick having local roots faceted by age and gender. Estimates are calculated from stan_m7. Open circles indicate point estimates. Lines denote 95% confidence intervals. There are considerable differences in the treatment effects between the various subgroups. The reported average treatment effect is 0.7 for no information. This figure shows that the average treatment effect varies in both directions for the subgroups. When no  behavioral information is present, the local roots effect ranges from 0.13 (Females 18-24) to 1.16 (Female 50-64). We can also see the diminishing effect when behavioral information is included as the highest effect across subgroups for Constituent focus is 0.8 while for Westminster focus is 0.9. Between genders, there does not appear to be signficant differences; however, for both Males and Females the older age groups have much larger local roots effects."}

# new model with interactions to subset the into gender and age groups 

stan_m7 <- stan_glm(nickminusphil ~ localtreat*(behtreat + gender*agegrp),
          data = d, refresh = 0)



#print(stan_m7, digits = 3, detail = FALSE) 

# splitting into two datasets. I arrange so that the two will eventaully line up
# when I bring them back together. I split so I can make predictions on all
# those with local roots and then all those without local roots

non_local<-newdf_2%>%
  arrange(behtreat, agegrp, gender)%>%
  filter(localtreat == "No local roots")

local<-newdf_2%>%
  arrange(behtreat, agegrp, gender)%>%
  filter(localtreat == "Local roots")


# making predictions for each of the subsets 

local_preds<-posterior_linpred(stan_m7, newdata = local, transform = TRUE)

non_local_preds<-posterior_linpred(stan_m7, newdata = non_local, transform = TRUE)

#  subtracting the two I get the average treatment effect

ame<- local_preds - non_local_preds

# since their are 4000 simulations for each row of possible combinations, I use
# these to reduce down to one predictions for each

avg_ame<-data.frame(predictions = colMedians(ame), mad = colMads(ame))

ame_full <-local%>%
  
  # no longer need this column as the table is now a treatment effect between
  # the two categories within local treatment
  
  select(-localtreat)%>%
  
  # adding the median effects 
  
  cbind(avg_ame)%>%
  
  # calculating the error 
  
  mutate(lower = predictions - 1.96*mad, upper = predictions + 1.96*mad)


# plotting the various results 

  ggplot(ame_full, aes(x = behtreat, y = predictions)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_linerange(aes(x=behtreat, ymin=lower, ymax=upper), size = 0.6) +
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  coord_flip() +
  
  # faceting to show the differences between the sub populations 
  
  facet_wrap(gender ~ agegrp, ncol = 4, nrow = 2) + 
  theme_bw() +
  theme(legend.position = "bottom") + 
  ggtitle("Average Treatment Effect of Local Roots Across \nGender and Age Subsets")



```



```{r, warning= FALSE, fig.cap = 'Estimated average marginal component effects of each MP attribute level compared to the baseline level of the attribute. In contrast to the original article, I subset the data by gender. Males pay slightly less attention to local roots than females. From the original study, MPs with any level of local roots were 10% and 13% more likely to be chosen by respondents. From my tests, males showed a lower response of only about a 10% increase for the 20 year as well as grew up in the area levels of local roots whereas the effect on females exceeded 15 %. The change in probability of a candidate being preferred due to local roots for men was about 1/3 less than that for women.'}
### Load in Study 2 data 

long.dat <- readRDS("./friends_neighbors_replication/study2data_long.rds")
wide.dat <- readRDS("./friends_neighbors_replication/study2data_wide.rds")


### Create labels for plotting

# for full results

# more informative labels 

labels.full <- rev(expression(
  "", italic("Party & position (baseline = Labour left-wing)"), "Labour centre", "Conservative centre", "Conservative right-wing",
  "", italic("Local roots (baseline = lives elsewhere)"),"5 years in area", "20 years in area", "Grew up and lives in area", 
  "", italic("Constituency work (baseline = 1 day)"), "2 days", "3 days", "4 days",
  "", italic("Main policy influence (baseline = party)"), "constituents' views","own personal views",
  "", italic("Policy interests (baseline = economy and tax)"), "education and health",
  "", italic("MP sex (baseline = female)"), "male"))

# for analysis of local roots only
labels.sub <- expression("Grew up and lives in area", "20 years in area", "5 years in area", 
                         italic("Local roots (baseline = lives elsewhere)"))


# label for x axis

effect.label <- "Change in probability of MP being preferred,\n relative to baseline"


### Figure 3: AMCEs for all attributes

# calling amce.tab to get the amce values for all these variables when used as
# explanatory variable for mp.preferred




 gender_amces<-long.dat%>%
  group_by(gender)%>%
  nest()%>%
  mutate(amce = map(data, ~amce.tab(data = ., variables = c("mp.partypos", "mp.localroots", "mp.const", "mp.influence", "mp.policy", "mp.gender"))))%>%
   select(amce, gender) %>%
   unnest(cols = c(amce)) %>%
   group_by(gender)%>%
   slice(-1) %>%
   filter(level != "baseline")


  ggplot(gender_amces, aes(x = category, y = AMCE, color = attribute)) + 
  
  # baseline of 0 to show how significant each coefficient is 
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # gives the standard error range and a point at the amce
  
  geom_pointrange(aes(ymin = ci.lo, ymax = ci.hi), size = 0.75) +
  
  # inputs better label for y axis (Will become x axis)
  labs(y = effect.label, x = "") + 
  
  # With the categories on x axis, everything is jumbled, thus flipping the
  # coordinates so the categories are legible
  
  coord_flip() + 
  
  # adding nice formating 
  
  theme_bw() + 
  theme(axis.text = element_text(colour = "black")) +
  theme(legend.position = "none") +
  theme(text = element_text(size = 8)) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 1), # remove ticks and justify
        axis.title.x = element_text(size = 10, vjust = 0)) + 
  
  # change labels from those from the dataset that were abbreivated to something
  # more infomrative
  
  scale_x_discrete(labels=labels) +ggtitle("Difference In Local Roots AMCE Between Genders") + facet_wrap(~gender, nrow = 1)
  
```

```{r, warning = FALSE, fig.cap= 'Estimated average marginal component effects of each MP attribute level compared to the baseline level of the attribute. In contrast to the original article, I subset the data by age. The results between the age groups were much more varied and there was not a distinct trend in the older a voter is the more likely they will chose a candidate based on local roots. All the results agreed with those of the first article in that the magnitude of the local roots effect is constistanlty amongst the largest across all the attributes'}




 age_amces<-long.dat%>%
  group_by(age_grouped)%>%
  nest()%>%
  mutate(amce = map(data, ~amce.tab(data = ., variables = c("mp.partypos", "mp.localroots", "mp.const", "mp.influence", "mp.policy", "mp.gender"))))%>%
   select(amce, age_grouped) %>%
   unnest() %>%
   group_by(age_grouped) %>%
   slice(-1) %>%
   filter(level != "baseline")
 
 
 
ggplot(age_amces, aes(x = category, y = AMCE, color = attribute)) + 
  
  # baseline of 0 to show how significant each coefficient is 
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # gives the standard error range and a point at the amce
  
  geom_pointrange(aes(ymin = ci.lo, ymax = ci.hi), size = 0.75) +
  
  # inputs better label for y axis (Will become x axis)
  labs(y = effect.label, x = "") + 
  
  # With the categories on x axis, everything is jumbled, thus flipping the
  # coordinates so the categories are legible
  
  coord_flip() + 
  
  # adding nice formating 
  
  theme_bw() + 
  theme(axis.text = element_text(colour = "black")) +
  theme(legend.position = "none") +
  theme(text = element_text(size = 8)) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 1), # remove ticks and justify
        axis.title.x = element_text(size = 10, vjust = 0)) + 
  
  # change labels from those from the dataset that were abbreivated to something
  # more infomrative
  
  scale_x_discrete(labels=labels) +ggtitle("Difference In Local Roots AMCE Between Genders") + facet_wrap(~age_grouped, nrow = 2, ncol = 2)

```






#Bibliography