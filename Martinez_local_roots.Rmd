---
title: 'Replication of "Why Friends and Neighbors? Explaining the Electoral Appeal of Local Roots"'
author: "Diego Martinez"
date: "4/3/2020"
output: bookdown::pdf_document2
bibliography: [bibliography.bib]



---

# Abstract 

Campbell et al (2019) show that candidate's with shared local roots appeals to voters and that having local roots is an important factor in a voter's candidate selection. I was successfully able to replicate all the results presented in the orignal article "Why Friends and Neighbors? Explaining the Electoral Appeal of Local Roots." by Rosie Campbell, Philip Cowley, Nick Vivyan, Markus Wagner. To further their analysis, I modeled how local roots influence different sub-populations including males vs. females voters as well as between different age groups. I found the average marginal effect of local roots to not remain constant across subgroups, meaning local roots affect groups of voters differently. Thus, any further research regarding local roots effects should consider the gender as well as age makeup of the sample.




They seek to understand why politicians with local roots have more successs by testing how this effect holds up when voters are treated with the candidate's behavioral information. They concluded that with more infomation, the local roots effect does diminish; however, it is still positve. Furthermore, voters had a marginally higher preference for candidates with local roots compared to other varying characterstics including gender, party, and policy interests. I was able to replicate all the results of their paper in R and all my code can be found in on Github. For my extension, I modeled how local roots influence different sub-populations including male vs. female voters as well as different age groups.   

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# With the time constrictions due to swithcing papers, I was not able to comment
# all of the replication code. I did comment areas I did need to make changes.

library(bookdown)
library(dplyr)
library(tinytex)
library(gt)
library(gtsummary)
library(stargazer)
library(knitr)
library(tidyverse)
library(devtools)
library(ggplot2)
library(sandwich)
library(lmtest)
library(stargazer)
library(cowplot)
library(cobalt)
library(xtable)
library(devtools)
library(margins)
library(car)
library(foreign)
library(rms)
library(reshape2)

library(naniar)
library(rstanarm)
library(tidybayes)
library(matrixStats)

```



# Introduction

"Why Friends and Neighbors? Explaining the Electoral Appeal of Local Roots" ^[[Replication Paper](https://www.journals.uchicago.edu/doi/abs/10.1086/703131)] takes a deeper look into the "friends and neighbors effect" first researched by Vladimer Orlando Key in 1949. The "friends and neighbors effect" has been researched over the years; however, the research has been been focused on observing its existence with the reasoning for why it occurs being untested and unknown. The prevailing prior conceptual understanding has been that voters use the fact a candidate has local roots to make assumptions about how a said candidate may represent their constituents. Campbell et al are the first to test this hypothesis, measuring how varying the level of information voter's receive about those running affects the candidate with local roots chances of being elected. They performed a vignette experiment of fake elections instead of analyzing a real election to isolate the effects of local roots and control for the amount of information, not allowing prior information about any candidates influence the results. They modeled respondents' satisfaction with candidates described in the experiment using linear regression, paying attention to the interaction term between local roots and behavioral information. They found results in favor of their hypthoesis as the local roots effect diminished in the presence of more information; however, the result being positive, they took a deeper look with a second study. After performing a conjoint survey experiment, they measured the average marginal component effect, change in probability of a candidate being preferred compared to a baseline for each regressor variable, using an OLS model. The model showed candidates with local roots compared to no local roots to be substantially preferred with this difference being one of the largest out of all the attributes analyzed. Thus, the reasoning for why this "friends and neighbors" effect occurs is not solely voters using local roots to make inferences, suggesting local roots to be important in candidate selection.

I utilized R, a language and environment for statistical computing, for the replication of the author's code [@r]. The data and code for the paper were made available on  Dataverse [@campbell_cowley_vivyan_wagner_2018]. I followed the format laid out by Gary King in "Publication, Publication" to guide my replication project [@king]. I use many techniques from the Gov 1006 textbook "Regression and Other Stories" [@RAOS] as well. I was able to succesfully replicate all the results of the paper. I came across a few errors based on the updating of packages; however, these were easily fixed. I had the most difficulty with trying to replicate the paper's stargazer table. The code by itself did not have the note at the bottom nor the column headers. I was able to replicate the results displayed in the table; however, the style and formatting were not exactly the same. All of the others code as well as my own can be found in my Github repository ^[[Github Reposotory](https://github.com/diegomartinez1221/gov_1006_replication)]. 

I found the results of the original paper very intriguing; however, my immediate question was how would the results hold across subsets of the sample population. Different age groups as well as genders display different preferences in political issues and voting trends, thus, should it be expected that they react to local roots in the same way? I began by simplying the regression model utilized by the authors in study 1 by excluding the education and social grade variables ^[After performing leave one out cross validation, loo, I found a model without these variables to be just as predictive, meaning these were only adding noise]. I also performed my analysis within a Bayesian framework, using stan_glm instead of just lm for my modeling. In order to look at how local roots affects subsets differently, I interacted the local roots treatment variable with the gender and age group variables. The average local roots treatment effect were slightly different across genders; however, the effect varied signicantly between age groups with the effect being stronger for older voters.

To continue with this theory, I took a look also extended my analysis to the study 2 data. Being a conjoint survey experiment, the best method in which to model such data is using an OLS model. Thus, the only way to compare the local roots effects across genders and age groups was to filter and subset the data prior to running their model. I the change in probability of MP being preferred due to local roots for men to be about 1/3 less than that for women. In between age groups, I saw similar results to those I found in my extension of study 1. The older voters showed a much stronger affinite for local roots, while younger voters change in probability was more affected by changes in the levels of behavioral information. 



# Literature Review 

The local roots effects has been well studied since 1949. Valdimer Orlando Key was the first to study this for Presidential Elections in the southern USA, dubbing his findings as the "friends and neighbors effect" [@Key]. In analysing Arkansas, he found that in their one party political system that the best explanation for support was just where you are from and who you know. Key himself was concerned by his own findings stating that candidates "can gain support, not primarily for what he stands for or because of his capacities, but because of where he lives‚Äù [@Key] . However, what Key found was only observational evidence that "candidates for state office tend to poll overwhelming majorities in their home counties" [@Key]. This friends and neighbors effect was not quantitatively highlighted until 1973 in an analysis of George Wallace's support across his govenitorial elections. In their analysis, counties of Alabama were classified as friends and neighbor counties were ones he had a disprortionate level of support and they all happened to be located around his home county in southeastern Alabama [@black_black_1973]. They found "evidence for the persitence of a strong degree of localism in politics" due to the importance of the the friends-and-neighbors indicator variable in their regression models [@black_black_1973]. A year later Raymond Tatalovich confirmed many of the previous findings in his research of Mississippi as well as made extensions across different types of elections. He found that the most prestigious elections in Mississippi (Senate Races) in fact had the most "friends and neighbors" voting [@tatalovich_1975]. Furthermore, important to my replication paper, he found that "increased visbility of candidates does serve to blunt thier friends and neighbors support" [@tatalovich_1975]. 
  
At this point, there was criticism as to the validity of this theory as their had not been anything larger than statewide elections tested and all the testing so far was on Southern states in the USA. The question was were these friends and neighbors results generalizable. Michael Gallagher was the first to document Key's theories outside the US in the importance of localism in candidate selection in Ireland [@gallagher]. Michael Lewis-Beck, Michael and Tom Rice were the first to look at localism on the national level of a presidential election, testing the true presence of a home state advantage. Not only did they find that there is such a thing, but they also most interestingly found that this has not diminished over time even with the nationalization of elections and greater ease with which candidates were able to reach voters across the USA [@lewis-beck_rice_1983]. The local advantage still endures. A myrad of further studies have been conducted on varying levels of elections from local to state wide to nation wide in a fair amount of different countries including Australia, Canada, Britain, Germany Estona, and Norway. The local roots/"friends and neighbors effect" appears has been proven to be far more than just a local phenomenon in one particular part of the world.
    
Other as well as the more recent studies have focused on the causes of the friends and neighbors effect instead of just proving its existence. Shaun Bowler Todd Donovan and Joseph Snipp believed the friends and neighbors effect is indirect, coming just out of having more information about candidates due to greater local media coverage for local candidates. Thus, there conclusion were opposed to previous understanding that people use local roots to evaluate candidates. However, they find localism merely comes out of "a voter‚Äôs proximity to local media sources" and how "knowledge about candidates is distributed" [@bowler_donovan_snipp_1993]. Similar results were also found in the UK that "the effects result more from the distribution of information than "from the hometown boy infuence" Key origonally proposed [@johnston_wickham-jones_pattie_cutts_pemberton_2016]. However, nothing has been definitive with studies still showing from surveys responses that voters exhibit "small differences in the ratings of candidates in response to sex, religion, age and education cues but more sizeable effects are apparent for the candidate's occupation and place of residence" [@campbell_cowley_2013]. 

Thus, with conflicting evidence, at the time of the paper I am analzying, there was still much unkown about the nature of the friends and neighbors effect that the authors sought to finally answer. Their explanation for the "direct effect of local roots on voter evaluations of a politician" is that "voters rationally use local roots as a low-cost cue for making inferences about a politician‚Äôs ‚Äúbehavioral localism" [@campbell_cowley_vivyan_wagner_2019]. This idea of behavioral localism was defined as a politician's willingness to act in the local communities interest [@shugart_valdini_suominen_2005]. This is because voters believe that candidates with local roots "are more emo- tionally connected to the constituency and better informed about constituents‚Äô needs [@shugart_valdini_suominen_2005]. Voters tend to believe local candidates share ideologies, experiences, and will have greater concern for them, thus, they receive the local support out of mere inference. This theory is finally tested in the article I analyze, directly testing whether or not local roots effect is just a filler for lack of other information. They do so by treating voters with behavioral localism information. The quantity measured is the presence as well as the change of magntitude in the friends and neighbors effect when voters are given more information.
    

# Paper Review 

My replication paper will be looking at Rosie Campbell, Philip Cowley, Nick Vivyan, Markus Wagner's paper, "Why Friends and Neighbors? Explaining the Electoral Appeal of Local Roots" ^[[Replication Paper](https://www.journals.uchicago.edu/doi/abs/10.1086/703131)] published in The Journal of Politics. The paper studies the phenomon of why local politicians seem to gain more electoral support in elections. Whereas the "friends and neigbors" effect has previously been found to arise when there is a lack of information in regard to policy, the only thing voters know is they are local, this article's purpose is to explain the "direct effect of local roots on voter evaluations of a politician" [@campbell_cowley_vivyan_wagner_2019]. It is the authors belief that voters "use local roots as a low-cost cue for making inferences about a politician‚Äôs ‚Äúbehavioral localism" (extend to which an elected official acts in the interest of their local consitutients)[@campbell_cowley_vivyan_wagner_2019]. Their hypothesis is that the power of local roots in decision making for voters is weakened when voters are more informed about the behavioral localism. The authors use survey experiments of made up candidates running for Parliament in the UK rather than performing an observational study on real elections. Thus, they are able to isolate the effects of local roots, ensure prior knowledege is not a factor. Everyone in the experiment will have the same level of knowledge about each candidate given to them in biographical descriptions through the survey. During real elections, a cofounding variable of the local roots effect is disproportionate amount of local media coverage for local candidates leading to easier recognition. 

Study 1 used a vignette experiment to more generally test whether the presence of information on behavioral localism impacts the effects of local roots while Study 2 gives people a more wide array of information to test whether having greater knowledge of each candidate eliminates the effects of local roots. Study 1 presented a hypothetical election between two candidates, testing how varying levels of behavioral localism information interacts with being a local candidate. The election was between Phillip (the constant), who moved to the area five years ago, and Nick whose localness was subject to change and assigned randomly. The experiment was set up in such a way that respondents were randomly given either no behavioral information or a short description for each.. When behavioral information was provided, Nick's behavior also varied between low and high behavioral localism while Phillip always had a constant average behavioral localism (splits time each week between dealing with local issues vs. dealing with national issues). With the information provided, respondants were asked on a scale from 0 to 10 (10 being most satisfied) with having each candidate as their elected official. The depedent variable for the models was Nick's score minus Phillip's. The authors modeled this with a least squares regression model and their focus was on the interaction term (being a local candidate and having information). The results were that the presence of behavioral information whether positive or negative for Nick does lessen the friends and neighbors effect, agreeing with their hypothesis that local roots is a cue when no other information is provide. However, they found Nick being a local candidate still positively impacts voters. The authors thus took a deeper look into this with study 2. 

In study 2, every respondent was treated with behavioral localism, local roots, and other varying characteristics such as gender, political interests, and party. Thus, in contrast to study 1, this will test if the local roots effect is nullfied in the candidate selection process when a wealth of other knowledege is presented to the voters, leaving them without the need to use local roots as a cue for the unkown. Each respondent was given five hypothetical elections with randomly varying attributes, all of which were more specific than in study one. The respondents were asked to pick wihch candidate they preferred based on information about the localness of the candidates, behavioral localism measured by on how much time is spent on local issues and where the candidates allegiance lies (national party, constituents, or personal views), and other defining characterstics like age, gender, and policy interests. Each respondent was given five varying elections to chose from. Since study 2 posseses much more and much more varied information on the candidates in question, they analyze the average marginal component effect (AMCE). This measures the probability of why a candidate is being chosen based on the candidate's particular attribute for each category compared to a baseline. This will present the change in probability of a candidate being preferred broken down by each possible indivual component of their profile. Even here with so much more information, they found the AMCE for being local was positive and one of the most prominent compared to other categories as well as the baseline of not being local. Thus, the authors empirically found local roots to be an important aspect for voter's in candidate selection and it is not merely cue. 



# Beautiful Graphic 


```{r, m5 graphic, include=FALSE, eval=FALSE}

# necessary packages. I needed to detach plyr because even though the author
# uses it, it interferes with my use of dplr


# reading in the data for study 1 

d <- readRDS("./friends_neighbors_replication/study1data.rds")

d%>%
  
  # grouping by both treatments variables, creating six groups to find the
  # average score for each candidate. These follows the same groupings used in
  # the article.
  
  group_by(localtreat, behtreat)%>%
  summarise(avg_phil_score = round(mean(philscore), digits = 2),
         avg_nick_score = round(mean(nickscore), digits = 2))%>%
  
  # right now the scores are in two different columns. Pivoting to bring these
  # two together so that I may compare then in a graphic. 
  
  pivot_longer(cols = ends_with("score"), 
               names_to = "candidate",
               values_to = "scores")%>%
  
  # many of the variables names are are currently appreviated or just not in a
  # pretty form for the grap. I use recode to change the variables to better
  # labels in the graph
  
  mutate(candidate = fct_recode(candidate, "Phil" = "avg_phil_score", "Nick" = "avg_nick_score"), 
         behtreat = fct_recode(behtreat, "Constituent focus" = "Const. focus", "National focus" = "Westmin. focus", "No Information" = "No behavioural info"))%>%

  # creating the plot. Chose columns just to be able to see the differences
  # between phil and nick as well as be able to see between the various facets
  
  ggplot() + geom_col(aes(y = scores, x = candidate, fill = candidate), width = 0.5)+
  
  # I use a grid to create a 3x2 graphic for the various treatment combinations. 
  
  facet_grid(behtreat~localtreat) + 
  
  # adding informative labels and titles 
  
  labs(title = "Average Candidate Scores Across Various Treatment Groups",
       subtitle = "Nick Having Local Roots Increases His Average Score Across All Levels of Behavioral\n Localism", x = "Candidate", y = "Scores", caption = "Footnote: Very similar to Figure 1 from `Why Friends and Neighbors? Explaining the Electoral Appeal of Local\n Roots`. I use the raw scores instead of Nick minus Phil for my graphic; however the same results can be seen.\nThe facets of local root vs. not local roots and the behavioral information are all refering to changes to Nick, not Phil,\n and these are the differences in treatment groups") + 
  
  # there is no need for the legend so I take it out to save room. 
  
  theme(legend.position = "none")
```

# Paper Extensions 

The authors were very thorough in their analysis including a lengthy appendix with a lot of other things that were tested that were not highlighted prominently in the main article. However, I do believe there are a of interesting paths to follow in regard to extensions. The authors in their discussion and conclusion provide a lengthy list of questions and extension that arise from their own analysis. However, most of this does involve trying to explain the local roots effect that was out of the scope of their analysis :"What might account for the local roots effects that we have observed but that are left un-explained by the particular mechanisms considered in this article?" [@campbell_cowley_vivyan_wagner_2019]. They go on to speak about theories as well as experiments that should be carried out, including questions and frameworks for experiments with real elections to view how their results play out in an actual race. However, they do not mention much at all about the shortcomings of their analysis (not that there is necessarily any, the analysis was very thorough) and all of these proposed extensions are outside the scope of Gov 1006. My extensions will be focused on the data and experiments they used and will consist of subsetting the data to view how their results apply to various subgroups of the population. 

For study 1, the authors in one of the appendices performed balance and randomization checks to insure that there is a "distributional balance of four respondent characteristics ‚Äì gender, age, education and social grade, all measured pretreatment ‚Äì across the six treatment groups created in the experiment" [@campbell_cowley_vivyan_wagner_2019]. Since "None of the differences in proportions across treatment groups are bigger than 10% and only a small number are greater than 5%" [@campbell_cowley_vivyan_wagner_2019], I believe this presents an excellent opportunity to dive deeper into if the "friends and neighbors" effect is stronger for one subset of the population than others (ex. males vs. females, older vs. younger). I hypothesis that there may in fact be some interesting differences especially in regard to age. For example, do the younger people in the study, who may not be as engaged in politics or care how they are represented in Parliament, more often vote for the candidate with local roots. Or even on the opposite side of the spectrum, do the elder people in the study who have been engrained in their community and care about their representation vote for the person who will be concerned more with the wants and needs of the people than his/her personal political views. I could do a lot more analysis such as this with gender as well and the other personal information included about the people surveyed. 




# Appendix

## Replication Appendix 


```{r, table 1 setup}
# Replication code for Study 1 in 
# Campbell, R., Cowley, P., Vivyan, N & Wagner, M. 
# 'Why friends and neighbors? Explaining the electoral appeal of local roots'.
# Journal of Politics


### Load in Study 1 data

d <- readRDS("./friends_neighbors_replication/study1data.rds")


### Table 2

# The authors created various different models of the data. These models are
# displayed in table 1 of the paper and model 4 is used throughout the paper.
# The authors same the results of the models and the errors in separate objects
# so that they can all be called togethered to create a stargazer table

m1 <- lm(nickminusphil ~ localtreat*behtreatsimple, data = d)

se1 <- sqrt(diag(vcovHC(m1)))

m2 <- lm(nickminusphil ~ localtreat*behtreatsimple +
           gender + agegrp + socgrade + qual
         , data = d)
se2 <- sqrt(diag(vcovHC(m2)))

m3 <- lm(nickminusphil ~ localtreat*behtreat, data = d)
se3 <- sqrt(diag(vcovHC(m3)))

m4 <- lm(nickminusphil ~ localtreat*behtreat +
           gender + agegrp + socgrade + qual
         , data = d)
se4 <- sqrt(diag(vcovHC(m4)))


#loo(m1)


#stan_m3 <- stan_glm(nickminusphil ~ localtreat*behtreat, data = d, refresh = 0)

#stan_m4 <- stan_glm(nickminusphil ~ localtreat*behtreat +
         # gender + agegrp + socgrade + qual
         #, data = d, refresh = 0)


#stan_m5 <- stan_glm(nickminusphil ~ localtreat*behtreat + age + age*localtreat*behtreat
         #, data = d, refresh = 0)


#stan_m6 <- stan_glm(tnickgt ~ localtreat*behtreat +
          # gender + agegrp + socgrade + qual
         #, data = d_new, family = binomial, refresh = 0)


#stan_m7 <- stan_glm(tnickgt ~ localtreat*behtreat +
          # gender + agegrp 
         #, data = d_new, family = binomial, refresh = 0)




#l_1<-loo(stan_m3)

#l_2<-loo(stan_m4)

#l_3<-loo(stan_m5)

#loo_compare(l_3, l_2)

#d_new<- d %>% mutate(tnickgt = case_when(nickgt == "TRUE" ~ 1, TRUE ~0))
```

I was able to replicate all tables and graphics: 



```{r table 1, results="asis", message=FALSE, comment=FALSE}
## Output to table

# I needed to change/add many things to the original to get a more similar table
# to the one that apppars in the paper. 

stargazer(header = FALSE, 
          
          # calling all the linear models and errors created in teh previous chunk. 
          
          mget(paste0("m",1:4)),
          se = mget(paste0("se",1:4)),
          
          # latex was the only kind I could get to output in the pdf. It was
          # orignally type html
          
          type = "latex",
          
          # needed to add the title myself as well. 
          
          title = " Relative Ratings of MP Nick by Local Roots and Behavioral Information Treatments in Study", 
          
          # Could not find another way to add the two separations visible in
          # the table model. I needed to use these latex commands combined with
          # column.separate.
          
          column.labels=c('\\shortstack{Conditioning Effect of Any Behavioral\\\\ Localism Information}', '\\shortstack{Separate Conditioning Effects for\\\\ High and Low Behavioral Localism}'),
          column.separate = c(2,2),
          
          # both of these are needed to erase the dependent variable 
          
          dep.var.labels.include = FALSE,
          dep.var.caption = "",
          #intercept.bottom = FALSE, intercept.top = TRUE,
          keep.stat = c("n", "rsq", "adj.rsq"),
          omit = "socgrade|agegrp|gender|qual",
          order = c("Constant", "^localtreatLocal roots$", 
                    "^behtreatsimpleBehavioural info$", 
                    "^behtreatConst. focus$", 
                    "^behtreatWestmin. focus$",
                    "^localtreatLocal roots:behtreatsimpleBehavioural info$",
                    "^localtreatLocal roots:behtreatConst. focus$",
                    "^localtreatLocal roots:behtreatWestmin. focus$"
          ),
          add.lines = list(c("Controls for voter characteristics?", 
                             rep(c("No","Yes"), 2))),
          covariate.labels = c("Intercept", "Local roots", "Behavioral localism information",
                               "Behavioral localism: High (vs. no info)",
                               "Behavioral localism: Low (vs. no info)",
                               "Local roots X Behavioral info.", 
                               "Local roots X High behavioral localism",
                               "Local roots X Low behavioral localism"),
          
          # The notes section was not originally included in the replication
          # code. I was able to create something similar using the latex
          # commends, but it is not an exact match
          
       notes.append = FALSE, notes.align = "l",
      notes.label  = "\\multicolumn{5}{l}{\\parbox[t]{\\textwidth}{Note. All models estimated via ordinary least squares. Dependent variable is respondent relative rating of MP Nick (the 0‚Äì10 rating of Nick minus that of Philip). Robust standard errors in parentheses. N p 5,203.}}\\\\",
      single.row = TRUE,
      column.sep.width = "1pt")

```


```{r Extension, eval=FALSE, include=FALSE}

newdf_2 <- data.frame(expand_grid(localtreat = unique(d$localtreat), behtreat = unique(d$behtreat), gender = unique(d$gender)),
                      socgrade = factor(levels(d$socgrade)[which.max(table(d$socgrade))], 
                  levels = levels(d$socgrade)),
qual = factor(levels(d$qual)[which.max(table(d$qual))], 
              levels = levels(d$qual)),
agegrp = factor(levels(d$agegrp)[which.max(table(d$agegrp))], 
                levels = levels(d$agegrp)))



newdf_2 <- data.frame(expand_grid(localtreat = unique(d$localtreat), behtreat = unique(d$behtreat), agegrp = unique(d$agegrp), gender = unique(d$gender)),
                      socgrade = factor(levels(d$socgrade)[which.max(table(d$socgrade))], 
                  levels = levels(d$socgrade)),
qual = factor(levels(d$qual)[which.max(table(d$qual))], 
              levels = levels(d$qual)))
# using the function created above to get predictions for each of the rows of
# newdf. The output of pred.rob saved in preds is a list. The next step is
# adding those values into the newdf dataframe to make the second part of
# graphic 1.

preds <- predict.rob(m4, vcov = vcovHC(m4), newdata = newdf_2)
newdf_2$yhat <- preds$fit
newdf_2$se.yhat <- preds$se.fit
newdf_2$lo <- newdf_2$yhat - (1.96*newdf_2$se.yhat)
newdf_2$hi <- newdf_2$yhat + (1.96*newdf_2$se.yhat)
predlevels.m4 <- newdf_2


predlevels.m4<-predlevels.m4%>%
  group_by(localtreat, behtreat, gender)%>%
  summarise(yhat = mean(yhat),
            se.yhat = mean(se.yhat),
            lo = mean(lo),
            hi = mean(hi))
  






## make predlevels plot

# releveling the variables to be more informative in the graphic. Most were
# abbreviated to save space in the original dataset

predlevels.comb <- predlevels.m4
predlevels.comb$behtreat.neat <- car:::recode(predlevels.comb$behtreat, 
                                              '"No behavioural info" = "Behavioral information treatment --\\nNo information (Vignettes 1-2)";
                                              "Westmin. focus" = "Behavioral information treatment --\\nLow behavioral localism (Vignettes 5-6)";
                                              "Const. focus" = "Behavioral information treatment --\\nHigh behavioral localism (Vignettes 3-4)"')
                                              #as.factor.result = TRUE)
predlevels.comb$behtreat.neat <- factor(sub(" --", ":", predlevels.comb$behtreat.neat),
                                        levels = c("Behavioral information treatment:\nNo information (Vignettes 1-2)", 
                                                   "Behavioral information treatment:\nLow behavioral localism (Vignettes 5-6)",
                                                   "Behavioral information treatment:\nHigh behavioral localism (Vignettes 3-4)"
                                        ))
# second plot is pretty much identical to the first plot created. This one is
# just with predicted values for the most common voter profile instead of real
# data.

ggplot(predlevels.comb, aes(x = localtreat, y = yhat)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_linerange(aes(x=localtreat, ymin=lo, ymax=hi), size = 0.6) +
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  coord_flip()+
  facet_grid(behtreat ~ gender) + 
  theme_bw() +
  theme(legend.position = "bottom") + 
  labs(title = "(a) Predicted relative rating")


#### group by AGE 





newdf_2 <- data.frame(expand_grid(localtreat = unique(d$localtreat), behtreat = unique(d$behtreat), agegrp = unique(d$agegrp), gender = unique(d$gender)),
                      socgrade = factor(levels(d$socgrade)[which.max(table(d$socgrade))], 
                  levels = levels(d$socgrade)),
qual = factor(levels(d$qual)[which.max(table(d$qual))], 
              levels = levels(d$qual)))
# using the function created above to get predictions for each of the rows of
# newdf. The output of pred.rob saved in preds is a list. The next step is
# adding those values into the newdf dataframe to make the second part of
# graphic 1.

preds <- predict.rob(m4, vcov = vcovHC(m4), newdata = newdf_2)
newdf_2$yhat <- preds$fit
newdf_2$se.yhat <- preds$se.fit
newdf_2$lo <- newdf_2$yhat - (1.96*newdf_2$se.yhat)
newdf_2$hi <- newdf_2$yhat + (1.96*newdf_2$se.yhat)
predlevels.m4 <- newdf_2




predlevels.m4%>%
  group_by(localtreat, behtreat, agegrp)%>%
  summarise(yhat = mean(yhat),
            se.yhat = mean(se.yhat),
            lo = mean(lo),
            hi = mean(hi))%>%
  group_by(behtreat, agegrp)

  

#### group by AGE 


## make predlevels plot

# releveling the variables to be more informative in the graphic. Most were
# abbreviated to save space in the original dataset

predlevels.comb <- predlevels.m4
predlevels.comb$behtreat.neat <- car:::recode(predlevels.comb$behtreat, 
                                              '"No behavioural info" = "Behavioral information treatment --\\nNo information (Vignettes 1-2)";
                                              "Westmin. focus" = "Behavioral information treatment --\\nLow behavioral localism (Vignettes 5-6)";
                                              "Const. focus" = "Behavioral information treatment --\\nHigh behavioral localism (Vignettes 3-4)"')
                                              #as.factor.result = TRUE)
predlevels.comb$behtreat.neat <- factor(sub(" --", ":", predlevels.comb$behtreat.neat),
                                        levels = c("Behavioral information treatment:\nNo information (Vignettes 1-2)", 
                                                   "Behavioral information treatment:\nLow behavioral localism (Vignettes 5-6)",
                                                   "Behavioral information treatment:\nHigh behavioral localism (Vignettes 3-4)"
                                        ))
# second plot is pretty much identical to the first plot created. This one is
# just with predicted values for the most common voter profile instead of real
# data.

ggplot(predlevels.comb, aes(x = localtreat, y = yhat)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_linerange(aes(x=localtreat, ymin=lo, ymax=hi), size = 0.6) +
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  coord_flip()+
  facet_grid(agegrp ~ behtreat) + 
  theme_bw() +
  theme(legend.position = "bottom") + 
  labs(title = "(a) Predicted relative rating")





predlevels.m4%>%
  group_by(localtreat, behtreat, agegrp)%>%
  summarise(yhat = mean(yhat))%>%
  group_by(behtreat, agegrp)%>%
  pivot_wider(names_from = localtreat, values_from = yhat)%>%
  mutate(AME = `Local roots` - `No local roots`)





margins.m4 <- out 


## make margins plot
margins.comb <- margins.m4

# changing the variables to be more informative in the graphic. Most were
# abbreviated to save space in the original dataset

margins.comb$behtreat.neat <- car:::recode(margins.comb$behtreat, 
                                           '"No behavioural info" = "Behavioral information treatment --\\nNo information (Vignettes 1-2)";
                                           "Westmin. focus" = "Behavioral information treatment --\\nLow behavioral localism (Vignettes 5-6)";
                                           "Const. focus" = "Behavioral information treatment --\\nHigh behavioral localism (Vignettes 3-4)"')
                                           #as.factor.result = TRUE)
margins.comb$behtreat.neat <- factor(sub(" --", ":", margins.comb$behtreat.neat),
                                     levels = c("Behavioral information treatment:\nNo information (Vignettes 1-2)", 
                                                "Behavioral information treatment:\nLow behavioral localism (Vignettes 5-6)",
                                                "Behavioral information treatment:\nHigh behavioral localism (Vignettes 3-4)"
                                     ))

# the first grahic has two parts that are set side by side. Thus, it is
# necessary to save the plot into an object p1.

p1 <-
  
  # plotting the average treatment effect when a candidate is local for each
  # level of behavioral info; however behavioral info is not yet incorporated
  # into the graph.
  
  ggplot(margins.comb, aes(x = factor, y = AME)) + 
  
  # line at 0 to show all are results are significant
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # show range of the effect 
  
  geom_linerange(aes(x=factor, ymin=lower, ymax=upper), size = 0.6) +
  
  # point estimate for the averages 
  
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  
  # flipping the coordinates because factors is currently all the same. Also
  # aesthetically more pleasing to go horizontally
  
  coord_flip() +
  
  # needing the facet wrap to add the info for each of the behavioral levels.
  # Currently everything was graphed on same axis and the lines were on top of
  # each other. This separates the treatment into each combo of local roots with
  # behavioral roots.
  
  facet_wrap( ~ behtreat.neat, ncol = 1) + 
  
  # manipulating aesthetics and adding a title for the final output.
  
  theme_bw() +
  theme(legend.position = "bottom") +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + 
  ggtitle("(b) Effect of MP local roots \ntreatment") 


```





```{r testing, include = FALSE, eval= FALSE}




colmea


out_stan <- summary(margins(stan_m5, vcov = vcovHC(stan_m5), 
                       at = list(behtreat = c("No behavioural info", "Const. focus", "Westmin. focus"))))
out <- subset(out, factor == "localtreatLocal roots")






d_new<- d %>% mutate(tnickgt = case_when(nickgt == "TRUE" ~ 1, TRUE ~0))


d %>% 
  mutate(pred_all = predict(stan_m5, d), residual = (nickminusphil-pred_all)^2) 

%>%
  summarise(mse = mean(sqrt(residual)))



d_new<- d %>% mutate(tnickgt = case_when(nickgt == "TRUE" ~ 1, TRUE ~0))

stan_m7 <- stan_glm(tnickgt ~ localtreat*behtreat +
           gender + agegrp 
         , data = d_new, family = binomial, refresh = 0)

stan_m7

x<-data.frame(x = colMeans(probs))

colM

newdf_2 <- data.frame(expand_grid(localtreat = unique(d$localtreat), behtreat = unique(d$behtreat), agegrp = unique(d$agegrp), gender = unique(d$gender)))

probs<-posterior_linpred(stan_m7, newdata = newdf_2, transform = TRUE)


test<-newdf_2%>%
  cbind(x)



ggplot(test, aes(x = agegrp, y = x, group = behtreat)) + 
  geom_jitter(height = 0.02) + stat_smooth(formula =  ~ aes(color = behtreat), method = "glm", method.args = list(family = "binomial"), se = TRUE) 


ggplot(d_new, aes(x = localtreat, y = tnickgt, group = behtreat)) + 
  geom_jitter(height = 0.02) + stat_smooth(data = d_new, formula = y ~ behtreat*localtreat + age + gender, aes(color = behtreat), method = "glm", method.args = list(family = "binomial"), se = TRUE) 



```



\hfill\break
\hfill\break
\hfill\break
\hfill\break
\hfill\break
\hfill\break


```{r figure 1, cache=TRUE}
### Figure 1

### Create useful functions

## Function to get predicted levels for different treatment combinations.


# function to make predictions from the linear regression on a person with
# different attributes. very much like postior_linpred.the Inputs for the
# function are a regression object, covariate matrix for all the vairables in
# the regression object, and the new data to perform predictions.

predict.rob <- function(object, vcov,newdata){
  
  # extracting info from the regression object
  
  tt <- terms(object)
  
  # check to make sure that the user input a new dataset
  
  if(missing(newdata)){ newdata <- x$model }
  else {
    
    # takes the response variable, the y, out of the terms object
    
    Terms <- delete.response(tt)
    
    # creates a dataframe were all the variables are constant save the local
    # treatment and behavioral treatment. Each row of the matrix is a different
    # combination of the two treatments.
    
    m <- model.frame(Terms, newdata, na.action = na.pass, 
                     xlev = object$xlevels)
    if (!is.null(cl <- attr(Terms, "dataClasses"))) 
      .checkMFClasses(cl, m)
    
    # turns the dataframe m into a dataframe of 1's and 0's for each row of m.
    # This now resembles how m will be fed into a regresson. For example the
    # Intercept is 1 for every row. However, for rows of m where there was an
    # interactions, the value will be 1 and when the interaction did not take
    # place, there will be a 0. Thus, it is a sort of expanded version of the
    # regression equation.
    
    X <- model.matrix(Terms, m, contrasts.arg = object$contrasts)
    
    # checks for if any of the variables are NULL from the original m matrix.
    # this adds in 0's for those instances where there are NULLs.
    
    offset <- rep(0, nrow(X))
    if (!is.null(off.num <- attr(tt, "offset"))) 
      for (i in off.num) offset <- offset + eval(attr(tt, 
                                                      "variables")[[i + 1]], newdata)
    if (!is.null(object$call$offset)) 
      offset <- offset + eval(object$call$offset, newdata)
    mmDone <- FALSE
  }
  #m.mat <- model.matrix(x$terms,data=newdata)
  
  # saving the coefficients from the model 
  
  m.coef <- object$coef
  
  # performing matrix multiplication to get the predicted value for each row,
  # which is each a set person with set attributes experiencing each of the
  # different treatments. Also does the same for the standard erorrs. 
  
  fit <- as.vector(X %*% object$coef)
  se.fit <- sqrt(diag(X%*%vcov%*%t(X)))
  
  # finally the function combines the results of fit and se.fit to lists as the
  # output.
  
  return(list(fit=fit,se.fit=se.fit))
}

## author: Avg treatment effect of local roots with const and westmihn. info

# the margins command will collect the average treatment effect for each of the
# covaraites at every level of behavioral info. This is the crux of the
# analysis. The one that is important to us is the avg effect for
# localtreatLocal roots. Thus subetting to only keep the effects when local
# treatment interacts with the levels of behavioral info.

out <- summary(margins(m4, vcov = vcovHC(m4), 
                       at = list(behtreat = c("No behavioural info", "Const. focus", "Westmin. focus"))))
out <- subset(out, factor == "localtreatLocal roots")

margins.m4 <- out 


## make margins plot
margins.comb <- margins.m4

# changing the variables to be more informative in the graphic. Most were
# abbreviated to save space in the original dataset

margins.comb$behtreat.neat <- car:::recode(margins.comb$behtreat, 
                                           '"No behavioural info" = "Behavioral information treatment --\\nNo information (Vignettes 1-2)";
                                           "Westmin. focus" = "Behavioral information treatment --\\nLow behavioral localism (Vignettes 5-6)";
                                           "Const. focus" = "Behavioral information treatment --\\nHigh behavioral localism (Vignettes 3-4)"')
                                           #as.factor.result = TRUE)
margins.comb$behtreat.neat <- factor(sub(" --", ":", margins.comb$behtreat.neat),
                                     levels = c("Behavioral information treatment:\nNo information (Vignettes 1-2)", 
                                                "Behavioral information treatment:\nLow behavioral localism (Vignettes 5-6)",
                                                "Behavioral information treatment:\nHigh behavioral localism (Vignettes 3-4)"
                                     ))

# the first grahic has two parts that are set side by side. Thus, it is
# necessary to save the plot into an object p1.

p1 <-
  
  # plotting the average treatment effect when a candidate is local for each
  # level of behavioral info; however behavioral info is not yet incorporated
  # into the graph.
  
  ggplot(margins.comb, aes(x = factor, y = AME)) + 
  
  # line at 0 to show all are results are significant
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # show range of the effect 
  
  geom_linerange(aes(x=factor, ymin=lower, ymax=upper), size = 0.6) +
  
  # point estimate for the averages 
  
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  
  # flipping the coordinates because factors is currently all the same. Also
  # aesthetically more pleasing to go horizontally
  
  coord_flip() +
  
  # needing the facet wrap to add the info for each of the behavioral levels.
  # Currently everything was graphed on same axis and the lines were on top of
  # each other. This separates the treatment into each combo of local roots with
  # behavioral roots.
  
  facet_wrap( ~ behtreat.neat, ncol = 1) + 
  
  # manipulating aesthetics and adding a title for the final output.
  
  theme_bw() +
  theme(legend.position = "bottom") +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + 
  ggtitle("(b) Effect of MP local roots \ntreatment") 

## predlevels for m4

# dataframe created for predictions for the predict.rob function we made
# earlier. The table will consistent of a combinations of all local roots and
# behavioral treatments levels (2,3 so 6 row in total). The table then then is
# filled with the most common value for each of the voter characterstics (most
# common age, gender...). Thus the table is one of the treatments effects on the
# most common of individual from the sample.

newdf <- data.frame(expand.grid(localtreat = factor(c("No local roots", "Local roots"), 
                                                    levels = levels(d$localtreat)),
                                behtreat = factor(levels(d$behtreat), levels = levels(d$behtreat))
),
gender = factor(levels(d$gender)[which.max(table(d$gender))], 
                levels = levels(d$gender)),
agegrp = factor(levels(d$agegrp)[which.max(table(d$agegrp))], 
                levels = levels(d$agegrp)),
socgrade = factor(levels(d$socgrade)[which.max(table(d$socgrade))], 
                  levels = levels(d$socgrade)),
qual = factor(levels(d$qual)[which.max(table(d$qual))], 
              levels = levels(d$qual))
)



# using the function created above to get predictions for each of the rows of
# newdf. The output of pred.rob saved in preds is a list. The next step is
# adding those values into the newdf dataframe to make the second part of
# graphic 1.

preds <- predict.rob(m4, vcov = vcovHC(m4), newdata = newdf)
newdf$yhat <- preds$fit
newdf$se.yhat <- preds$se.fit
newdf$lo <- newdf$yhat - (1.96*newdf$se.yhat)
newdf$hi <- newdf$yhat + (1.96*newdf$se.yhat)
predlevels.m4 <- newdf


## make predlevels plot

# releveling the variables to be more informative in the graphic. Most were
# abbreviated to save space in the original dataset

predlevels.comb <- predlevels.m4
predlevels.comb$behtreat.neat <- car:::recode(predlevels.comb$behtreat, 
                                              '"No behavioural info" = "Behavioral information treatment --\\nNo information (Vignettes 1-2)";
                                              "Westmin. focus" = "Behavioral information treatment --\\nLow behavioral localism (Vignettes 5-6)";
                                              "Const. focus" = "Behavioral information treatment --\\nHigh behavioral localism (Vignettes 3-4)"')
                                              #as.factor.result = TRUE)
predlevels.comb$behtreat.neat <- factor(sub(" --", ":", predlevels.comb$behtreat.neat),
                                        levels = c("Behavioral information treatment:\nNo information (Vignettes 1-2)", 
                                                   "Behavioral information treatment:\nLow behavioral localism (Vignettes 5-6)",
                                                   "Behavioral information treatment:\nHigh behavioral localism (Vignettes 3-4)"
                                        ))
# second plot is pretty much identical to the first plot created. This one is
# just with predicted values for the most common voter profile instead of real
# data.

p2 <-  ggplot(predlevels.comb, aes(x = localtreat, y = yhat)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_linerange(aes(x=localtreat, ymin=lo, ymax=hi), size = 0.6) +
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  coord_flip() +
  facet_wrap( ~ behtreat.neat, ncol = 1) + 
  theme_bw() +
  theme(legend.position = "bottom") + 
  labs(title = "(a) Predicted relative rating")


## Now combine aspects of above plots into 3 x 2 plot
pcomb <- plot_grid(p2, p1, align = "h", rel_widths = c(1.0,0.80))

```

```{r}
pcomb
```

\hfill\break
\hfill\break
\hfill\break
\hfill\break
\hfill\break
\hfill\break

```{r figure 2 functions}

    
## Function to get regression-based AMCE estimates for each attribute level using 
## OLS estimator with clustered SEs (Hainmueller, Hopkins and Yammamoto 2014)

# function called within amce.tab to get the actual amce's 

get.amcetab <- function(data, variables, J = 2){    
  
  # important to have the list of variables length to know how many AMCE's to
  # calculate
  
  Nvar <- length(variables)
  amce.list <- vector("list", length = Nvar)
  
  for(i in 1:Nvar){ # get AMCE for each variable attribute
    
    # regression on every variables with mp.preferred individually. fmla builds
    # each regression through each iteration of the loop
    
    fmla <- as.formula(paste("mp.preferred ~ ",variables[i], sep = ""))
    
    # fmla is placed into ols. Runs a linear regression based on the formula.
    # Gets same results for the regression as just calling lm but there are
    # other things in the output as well
    
    model <- ols(fmla, data = data, x = T, y = T) 
    # NOTE: The data for the model has to have no NAs on any model variables 
    # for the robcov(cluster()) function to work 
    
    #adjusts the variance-covariance matrix of a fit to correct for
    #heteroscedasticity and for correlated responses from cluster samples.
    #Clusters by ID, which is each individual person tested as they were tested
    #10 times.
    
    model.clus <- robcov(model, cluster = data$ID, method = "efron")
    
    # list of the coefficients from the model. taking out the baseline because
    # they do comparisons to the baseline
    
    coef <- model.clus$coef[names(model.clus$coef)!="Intercept"]
    
    # calculates as well as gets the standard error for each of the coefficients 
    
    se.clus <- sqrt(diag(model.clus$var))
    se.clus <- se.clus[names(se.clus)!="Intercept"]   
    
    # creating a table from the info drawn from the model 
    
    sub.tab <- data.frame("AMCE" = coef, 
                          "ci.lo" = coef - (1.96*se.clus),
                          "ci.hi" = coef + (1.96*se.clus),
                          "cluster.se" = se.clus)
    
    # making the name of each of the coefficients from the model a column in the
    # created dataframe
    
    sub.tab$category <- names(coef)
    
    # splits into two columns. Since these are categorical variables, the
    # rgression output is the name of the variable being regressed on, and equal
    # sign, and then the category. This separates the variable and the category
    # of the coefficient into two columns
    
    sub.tab <- cbind(sub.tab, colsplit(sub.tab$category, "=", c("attribute","level")))
    sub.tab$level <- as.character(sub.tab$level)    
    
    # no longer needed since made the two columns out of the variable names
    
    row.names(sub.tab) <- NULL
    
    ## add in gaps and baselines
    
    # these had been taken out when we did not want coefficient for the
    # intercept. Now it is added back into table for reference as 0's
    
    to.add <- data.frame(AMCE = c(NA,NA, 0), ci.lo = c(NA,NA, 0), ci.hi = c(NA,NA, 0),
                         cluster.se = c(NA,NA, 0),
                         category = rep("", 3), attribute = rep(sub.tab$attribute[1],3),
                         level = c("", " ", "baseline"))
    
    # since this is a for loop will do the same for each regression
    # individually. All are saved into amce.list
    
    amce.list [[i]] <- rbind(to.add, sub.tab)
  } 
  
  # create one large table from the list of individual subtables for each regression 
  
  amce.tab <- do.call("rbind", amce.list)
  
  # re-make initial labels column
  
  amce.tab$category <- paste(amce.tab$attribute, amce.tab$level, sep = ": ")
  
  # make this into ordered factor
  amce.tab$category <- factor(amce.tab$category, levels = rev(amce.tab$category), order =T)    
  
  return(amce.tab)
}


## author: Function that calls get.amcetab for multiple predictors and combines results

# inputs for the function are a dataframe, variables from dataframe wanting to
# be used to get their amce and whether there are multiple datasets, and if they
# test subjects are in same political party.

# amce.tab only function is to prepare data so as to call get_amcetab. That is
# why there are many if statements for different number of datasets and parties.

amce.tab <- function(data, variables, multi = F, same.party = F){
  # data must be a single data frame or a list of data frames (if multi = T)
  # with named elements
  # Also relies on specific ordering of explanatory variables

  # first try when multi is true. What is going on here is iterating over the
  # list of dataframes all calling get_amcetab for each of the dataframes in the
  # list. Necessary that all dataframes have the variables inputed into the
  # orignal function
  
  if(multi == T & same.party == F){
    amce.tab.list <- list(NA, length = length(data))
    for(i in 1:length(data)){
      tmp <- get.amcetab(data[[i]], variables = variables)
      tmp$set <- rep(names(data)[i], nrow(tmp))
      amce.tab.list[[i]] <- tmp
    }
    
    # combining all the dataframes together to create one large dataset with set
    # column to be able to tell which amce belongs to which dataset from the list.
    
    amce.tab <- do.call("rbind",amce.tab.list)
    amce.tab$set <- factor(amce.tab$set)
    return(amce.tab)
  }
  

  # same as before in needing to iterate over every dataset in the list of
  # datasets and call get_amcetab for all of them
  
  if(multi == T & same.party == T){
    amce.tab.list <- list(NA, length = length(data))
    for(i in 1:length(data)){
      
      # check for same party. If they are the same party. If true, the first
      # variable is taken out of the variable list. This is why author stated
      # that the order of variables is important.
      
      vars <- if(grepl("same party", names(data)[i])==T|grepl("Same Party", names(data)[i])==T) variables[2:length(variables)] else variables
      
      # regardless now calls get.amcetab for every element of the list
      
      tmp <- get.amcetab(data[[i]], variables = vars)
      tmp$set <- rep(names(data)[i], nrow(tmp))
      amce.tab.list[[i]] <- tmp
    }
    names(amce.tab.list) <- names(data)
    
    # separtes the data where they are same party and datasets where they
    # are not in same party
    diff.party <- amce.tab.list[grepl("same party", names(amce.tab.list))==F&
                                  grepl("Same Party", names(amce.tab.list))==F]
    same.party <- amce.tab.list[grepl("same party", names(amce.tab.list))==T |
                                  grepl("Same Party", names(amce.tab.list))==T]
    
    # dataframe to fill in when they are same party 
    
    to.add <- data.frame(AMCE = rep(NA, 4), ci.lo = rep(NA, 4), ci.hi =  rep(NA, 4),
                         cluster.se =  rep(NA, 4),
                         category =  diff.party[[1]]$category[1:4], 
                         attribute =  diff.party[[1]]$attribute[1:4], 
                         level = diff.party[[1]]$level[1:4],
                         set = rep(NA, 4))
    
    for(i in 1:length(data)){ 
      
      #replacing results from amcetab when same party is true.  
      
      if(grepl("same party", names(data)[i])==T|grepl("Same Party", names(data)[i])==T){
        amce.tab.list[[i]] <- rbind(to.add,amce.tab.list[[i]])
        amce.tab.list[[i]]$set[1:4] <-  amce.tab.list[[i]]$set[5:8]
      }
      else amce.tab.list[[i]] <- amce.tab.list[[i]]
    }    
    
    # finally combines all the datasets from the list of datasets together and
    # outputs the results together.
    
    amce.tab <- do.call("rbind",amce.tab.list)
    amce.tab$set <- factor(amce.tab$set)
    return(amce.tab)
  } 
  
  # last if statement and it is the easiest just when multi is false. All
  # amce.tab needs does is just call get.amcetab
  
  if(multi == F)   {
    
    get.amcetab(data, variables)
  }
  
}


```


```{r figure 2 graph, message=FALSE, warning=FALSE}
### Load in Study 2 data 

long.dat <- readRDS("./friends_neighbors_replication/study2data_long.rds")
wide.dat <- readRDS("./friends_neighbors_replication/study2data_wide.rds")


### Create labels for plotting

# for full results

# more informative labels 

labels.full <- rev(expression(
  "", italic("Party & position (baseline = Labour left-wing)"), "Labour centre", "Conservative centre", "Conservative right-wing",
  "", italic("Local roots (baseline = lives elsewhere)"),"5 years in area", "20 years in area", "Grew up and lives in area", 
  "", italic("Constituency work (baseline = 1 day)"), "2 days", "3 days", "4 days",
  "", italic("Main policy influence (baseline = party)"), "constituents' views","own personal views",
  "", italic("Policy interests (baseline = economy and tax)"), "education and health",
  "", italic("MP sex (baseline = female)"), "male"))

# for analysis of local roots only
labels.sub <- expression("Grew up and lives in area", "20 years in area", "5 years in area", 
                         italic("Local roots (baseline = lives elsewhere)"))


# label for x axis

effect.label <- "Change in probability of MP being preferred,\n relative to baseline"


### Figure 3: AMCEs for all attributes

# calling amce.tab to get the amce values for all these variables when used as
# explanatory variable for mp.preferred

res <- amce.tab(data = long.dat, 
                variables = c("mp.partypos", "mp.localroots", "mp.const", "mp.influence", "mp.policy", "mp.gender")
                , multi = F)

# reversing the order of the factors for the various categories of amce, unsure
# why

res$category <- factor(as.character(res$category), levels = rev(as.character(res$category)), order =T)
#write.csv(res, "amce-all.csv")# write results to csv file

# Full plot for all attributes

# since each had two spaces in between, the top one is not needed.

res <- res[2:nrow(res),] # chop off top empty layer
res <- subset(res, level != "baseline")# remove artificial 'baseline' rows

# labels created above

labels <- labels.full

# initiates the plot. The attribute column is each variable which is why every
# category within a variable has same color.

ggplot(res, aes(x = category, y = AMCE, color = attribute)) + 
  
  # baseline of 0 to show how significant each coefficient is 
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # gives the standard error range and a point at the amce
  
  geom_pointrange(aes(ymin = ci.lo, ymax = ci.hi), size = 0.75) +
  
  # inputs better label for y axis (Will become x axis)
  labs(y = effect.label, x = "") + 
  
  # With the categories on x axis, everything is jumbled, thus flipping the
  # coordinates so the categories are legible
  
  coord_flip() + 
  
  # adding nice formating 
  
  theme_bw() + 
  theme(axis.text = element_text(colour = "black")) +
  theme(legend.position = "none") +
  theme(text = element_text(size = 15)) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 1), # remove ticks and justify
        axis.title.x = element_text(size = 13, vjust = 0)) + 
  
  # change labels from those from the dataset that were abbreivated to something
  # more infomrative
  
  scale_x_discrete(labels=labels)
```


## Extension Appendix

```{r Stan Extensions, cache= TRUE}


stan_m3 <- stan_glm(nickminusphil ~ localtreat*behtreat, data = d, refresh = 0)

stan_m4 <- stan_glm(nickminusphil ~ localtreat*behtreat +
          gender + agegrp + socgrade + qual
         , data = d, refresh = 0)

stan_m5 <- stan_glm(nickminusphil ~ localtreat*behtreat + agegrp + gender
         , data = d, refresh = 0)


 stan_m6<- stan_glm(nickminusphil ~ localtreat*(behtreat+ gender+ agegrp)
         , data = d, refresh = 0)



# stan_m7 <- stan_glm(tnickgt ~ localtreat*behtreat +
#            gender + agegrp 
#          , data = d_new, family = binomial, refresh = 0)



l_1<-loo(stan_m3)

l_2<-loo(stan_m4)

l_3<-loo(stan_m5)

#(l_4<-loo(stan_m6))


newdf_2 <- data.frame(expand_grid(localtreat = unique(d$localtreat), behtreat = unique(d$behtreat), agegrp = unique(d$agegrp), gender = unique(d$gender)))

probs<-posterior_linpred(stan_m5, newdata = newdf_2, transform = TRUE)

predictions<-data.frame(predictions = colMedians(probs), mad = colMads(probs))




preds<-newdf_2%>%
  cbind(predictions)%>%
  mutate(lower = predictions - 1.96*mad, upper = predictions + 1.96*mad)

```

```{r}



preds %>%
  filter(agegrp == "25-49", gender == "Female")%>%
  # plotting the average treatment effect when a candidate is local for each
  # level of behavioral info; however behavioral info is not yet incorporated
  # into the graph.
  
  ggplot(aes(x = localtreat, y = predictions)) + 
  
  # line at 0 to show all are results are significant
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # show range of the effect 
  
  geom_linerange(aes(x=localtreat, ymin=lower, ymax=upper), size = 0.6) +
  
  # point estimate for the averages 
  
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  
  # flipping the coordinates because factors is currently all the same. Also
  # aesthetically more pleasing to go horizontally
  
  coord_flip() +
  
  # needing the facet wrap to add the info for each of the behavioral levels.
  # Currently everything was graphed on same axis and the lines were on top of
  # each other. This separates the treatment into each combo of local roots with
  # behavioral roots.
  
  facet_wrap( ~ behtreat, ncol = 1) + 
  
  # manipulating aesthetics and adding a title for the final output.
  
  theme_bw() +
  theme(legend.position = "bottom") +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())

```



```{r}

stan_m7 <- stan_glm(nickminusphil ~ localtreat*(behtreat + gender*agegrp),
          data = d, refresh = 0)



#print(stan_m7, digits = 3, detail = FALSE) 

newdf_2 <- data.frame(expand_grid(localtreat = unique(d$localtreat), behtreat = unique(d$behtreat), agegrp = unique(d$agegrp), gender = unique(d$gender)))

non_local<-newdf_2%>%
  arrange(behtreat, agegrp, gender)%>%
  filter(localtreat == "No local roots")

local<-newdf_2%>%
  arrange(behtreat, agegrp, gender)%>%
  filter(localtreat == "Local roots")



local_preds<-posterior_linpred(stan_m7, newdata = local, transform = TRUE)

non_local_preds<-posterior_linpred(stan_m7, newdata = non_local, transform = TRUE)

ame<- local_preds - non_local_preds


avg_ame<-data.frame(predictions = colMedians(ame), mad = colMads(ame))

local%>%
  select(-localtreat)%>%
  cbind(avg_ame)%>%
  mutate(lower = predictions - 1.96*mad, upper = predictions + 1.96*mad)%>% 
  ggplot(aes(x = behtreat, y = predictions)) + 
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  geom_linerange(aes(x=behtreat, ymin=lower, ymax=upper), size = 0.6) +
  geom_point(size = 3.5, shape = 21, fill = "white") +
  labs(x = "", y = "") + 
  coord_flip() +
  facet_wrap(gender ~ agegrp, ncol = 4, nrow = 2) + 
  theme_bw() +
  theme(legend.position = "bottom")



```



```{r Extension 2, warning= FALSE}
### Load in Study 2 data 

long.dat <- readRDS("./friends_neighbors_replication/study2data_long.rds")
wide.dat <- readRDS("./friends_neighbors_replication/study2data_wide.rds")


### Create labels for plotting

# for full results

# more informative labels 

labels.full <- rev(expression(
  "", italic("Party & position (baseline = Labour left-wing)"), "Labour centre", "Conservative centre", "Conservative right-wing",
  "", italic("Local roots (baseline = lives elsewhere)"),"5 years in area", "20 years in area", "Grew up and lives in area", 
  "", italic("Constituency work (baseline = 1 day)"), "2 days", "3 days", "4 days",
  "", italic("Main policy influence (baseline = party)"), "constituents' views","own personal views",
  "", italic("Policy interests (baseline = economy and tax)"), "education and health",
  "", italic("MP sex (baseline = female)"), "male"))

# for analysis of local roots only
labels.sub <- expression("Grew up and lives in area", "20 years in area", "5 years in area", 
                         italic("Local roots (baseline = lives elsewhere)"))


# label for x axis

effect.label <- "Change in probability of MP being preferred,\n relative to baseline"


### Figure 3: AMCEs for all attributes

# calling amce.tab to get the amce values for all these variables when used as
# explanatory variable for mp.preferred




 gender_amces<-long.dat%>%
  group_by(gender)%>%
  nest()%>%
  mutate(amce = map(data, ~amce.tab(data = ., variables = c("mp.partypos", "mp.localroots", "mp.const", "mp.influence", "mp.policy", "mp.gender"))))%>%
   select(amce, gender) %>%
   unnest(cols = c(amce)) %>%
   group_by(gender)%>%
   slice(-1) %>%
   filter(level != "baseline")


  ggplot(gender_amces, aes(x = category, y = AMCE, color = attribute)) + 
  
  # baseline of 0 to show how significant each coefficient is 
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # gives the standard error range and a point at the amce
  
  geom_pointrange(aes(ymin = ci.lo, ymax = ci.hi), size = 0.75) +
  
  # inputs better label for y axis (Will become x axis)
  labs(y = effect.label, x = "") + 
  
  # With the categories on x axis, everything is jumbled, thus flipping the
  # coordinates so the categories are legible
  
  coord_flip() + 
  
  # adding nice formating 
  
  theme_bw() + 
  theme(axis.text = element_text(colour = "black")) +
  theme(legend.position = "none") +
  theme(text = element_text(size = 8)) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 1), # remove ticks and justify
        axis.title.x = element_text(size = 10, vjust = 0)) + 
  
  # change labels from those from the dataset that were abbreivated to something
  # more infomrative
  
  scale_x_discrete(labels=labels) +ggtitle("Difference In Local Roots AMCE Between Genders") + facet_wrap(~gender, nrow = 1)+ labs(caption = "Estimated average marginal component effects of each MP attribute level compared to the baseline\n level of the attribute. In contrast to the original article, I subset the data by gender. \nMales pay slightly less attention to local roots than females.")
  


 age_amces<-long.dat%>%
  group_by(age_grouped)%>%
  nest()%>%
  mutate(amce = map(data, ~amce.tab(data = ., variables = c("mp.partypos", "mp.localroots", "mp.const", "mp.influence", "mp.policy", "mp.gender"))))%>%
   select(amce, age_grouped) %>%
   unnest() %>%
   group_by(age_grouped) %>%
   slice(-1) %>%
   filter(level != "baseline")
 
 
 
ggplot(age_amces, aes(x = category, y = AMCE, color = attribute)) + 
  
  # baseline of 0 to show how significant each coefficient is 
  
  geom_hline(yintercept = 0, linetype = "dashed", size = 0.5) +
  
  # gives the standard error range and a point at the amce
  
  geom_pointrange(aes(ymin = ci.lo, ymax = ci.hi), size = 0.75) +
  
  # inputs better label for y axis (Will become x axis)
  labs(y = effect.label, x = "") + 
  
  # With the categories on x axis, everything is jumbled, thus flipping the
  # coordinates so the categories are legible
  
  coord_flip() + 
  
  # adding nice formating 
  
  theme_bw() + 
  theme(axis.text = element_text(colour = "black")) +
  theme(legend.position = "none") +
  theme(text = element_text(size = 8)) +
  theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 1), # remove ticks and justify
        axis.title.x = element_text(size = 10, vjust = 0)) + 
  
  # change labels from those from the dataset that were abbreivated to something
  # more infomrative
  
  scale_x_discrete(labels=labels) +ggtitle("Difference In Local Roots AMCE Between Genders") + facet_wrap(~age_grouped, nrow = 2, ncol = 2)

```






#Bibliography